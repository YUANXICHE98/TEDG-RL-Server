#!/usr/bin/env python3
"""
TEDG-RL NetHackè®­ç»ƒ - V4ç‰ˆæœ¬
Cross-Attention Guided Hierarchical MoE

æ ¸å¿ƒç‰¹æ€§ (ç›¸æ¯”V3):
1. **Cross-Attentionèåˆ**: æ›¿ä»£concatï¼Œè®©ç¬¦å·ä¿¡æ¯ä¸»åŠ¨æŸ¥è¯¢è§†è§‰ä¿¡æ¯
2. **Sparse Attention Gate**: åªå…³æ³¨ç›¸å…³çš„è§†è§‰ç‰¹å¾
3. **Context Vector**: ç”Ÿæˆç´§å‡‘çš„256ç»´ä¸Šä¸‹æ–‡è¡¨ç¤º
4. **æ¨¡æ€å¹³è¡¡**: ç¼“è§£æ¨¡æ€ä¸»å¯¼é—®é¢˜

å…¶ä»–ç‰¹æ€§æ²¿ç”¨V3:
- GATæ¨ç†å±‚ - åŠ¨æ€æ¿€æ´»è¶…å›¾èŠ‚ç‚¹
- Sparsemaxè·¯ç”± - è½¯ä¸­å¸¦ç¡¬ï¼Œé¿å…å¡Œç¼©
- è¯­ä¹‰ä¸“å®¶ - Survival/Combat/Exploration/General
- ä¸‰é˜¶æ®µè®­ç»ƒ - Warmup â†’ Transition â†’ Fine-tune
- å¤šé‡ç¨³å®šæ€§æªæ–½ - è´Ÿè½½å‡è¡¡ã€å¤šæ ·æ€§ã€NaNæ£€æµ‹
- **æ‰€æœ‰è¾…åŠ©æŸå¤±å‡½æ•°**: Managerçº¦æŸã€è´Ÿè½½å‡è¡¡ã€ä¸“å®¶å¤šæ ·æ€§ç­‰

å®ç°ç­–ç•¥:
- ç›´æ¥å¯¼å…¥V3çš„è®­ç»ƒè„šæœ¬
- åªæ›¿æ¢ç½‘ç»œç±»ä¸ºV4çš„CrossAttentionMoEPolicy
- å…¶ä»–è®­ç»ƒé€»è¾‘ï¼ˆPPOã€æŸå¤±å‡½æ•°ã€ç›‘æ§ç­‰ï¼‰å®Œå…¨æ²¿ç”¨V3
"""

import os
import sys
import json
import time
from pathlib import Path

# å…è®¸ç›´æ¥è¿è¡Œ
_REPO_ROOT = Path(__file__).resolve().parents[2]
if str(_REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(_REPO_ROOT))

import torch
import torch.nn.functional as F
import numpy as np

# ============================================================================
# æ ¸å¿ƒç­–ç•¥: ç›´æ¥ä½¿ç”¨V3çš„è®­ç»ƒè„šæœ¬ï¼Œåªæ›¿æ¢ç½‘ç»œç±»
# ============================================================================

# 1. å¯¼å…¥V3çš„æ‰€æœ‰è®­ç»ƒç»„ä»¶
from ablation_v3.train.train_v3_gat_moe import (
    # è¾…åŠ©æŸå¤±å‡½æ•° (å®Œå…¨æ²¿ç”¨)
    load_balance_loss,
    expert_diversity_loss,
    aggregate_operators_to_experts,
    hypergraph_alignment_loss,
    enhanced_semantic_orthogonality_loss,
    expert_overlap_penalty,
    
    # è®­ç»ƒé…ç½® (å®Œå…¨æ²¿ç”¨)
    get_training_config,
    get_lr_scheduler,
    
    # ç›‘æ§å’Œè¯Šæ–­ (å®Œå…¨æ²¿ç”¨)
    TrainingMonitor,
    NaNDetector,
    RewardNormalizer,
    
    # å·¥å…·å‡½æ•° (å®Œå…¨æ²¿ç”¨)
    get_device,
    extract_atoms_from_obs,
    extract_state_from_obs,
    log_gradient_norms,
)

# 2. å¯¼å…¥V4ç½‘ç»œ (å”¯ä¸€çš„æ”¹åŠ¨)
from src.core.networks_v4_cross_attention import CrossAttentionMoEPolicy

# ============================================================================
# V4ç‰¹å®šçš„ç½‘ç»œåˆ›å»ºå‡½æ•° (å”¯ä¸€éœ€è¦ä¿®æ”¹çš„éƒ¨åˆ†)
# ============================================================================

def create_v4_policy(device, args):
    """
    åˆ›å»ºV4ç½‘ç»œ (Cross-Attention MoE)
    
    ä¸V3çš„åŒºåˆ«:
    - ä½¿ç”¨CrossAttentionMoEPolicyæ›¿ä»£GATGuidedMoEPolicy
    - æ–°å¢cross_attn_headså’Œsparse_topkå‚æ•°
    - å…¶ä»–å‚æ•°å®Œå…¨ä¸€è‡´
    
    Args:
        device: è®¾å¤‡
        args: å‘½ä»¤è¡Œå‚æ•°
    
    Returns:
        policy_net: V4ç­–ç•¥ç½‘ç»œ
    """
    print(f"\n[åˆå§‹åŒ–V4ç½‘ç»œ - Cross-Attention MoE]")
    
    policy_net = CrossAttentionMoEPolicy(
        hypergraph_path="data/hypergraph/hypergraph_gat_structure.json",
        state_dim=115,
        hidden_dim=256,
        action_dim=23,
        num_experts=args.num_experts,
        use_sparsemax=True,
        cross_attn_heads=4,      # V4æ–°å¢: Cross-Attentionå¤´æ•°
        sparse_topk=0.3          # V4æ–°å¢: ç¨€ç–æ³¨æ„åŠ›ä¿ç•™æ¯”ä¾‹
    ).to(device)
    
    # å†»ç»“GAT (å¦‚æœéœ€è¦)
    if args.freeze_gat:
        print("  â†’ å†»ç»“GATå‚æ•°")
        for param in policy_net.gat.parameters():
            param.requires_grad = False
    
    total_params = sum(p.numel() for p in policy_net.parameters())
    trainable_params = sum(p.numel() for p in policy_net.parameters() if p.requires_grad)
    print(f"âœ“ æ€»å‚æ•°: {total_params:,}, å¯è®­ç»ƒ: {trainable_params:,}")
    print(f"âœ“ Cross-Attention: 4 heads, sparse_topk=0.3")
    print(f"âœ“ Context Vector: 256ç»´ (vs V3çš„512ç»´)")
    print(f"âœ“ æ‰€æœ‰V3çš„è¾…åŠ©æŸå¤±å‡½æ•°å·²ä¿ç•™")
    
    return policy_net


def main():
    """V4ä¸»è®­ç»ƒå¾ªç¯ - æ²¿ç”¨V3çš„è®­ç»ƒé€»è¾‘"""
    import argparse
    
    parser = argparse.ArgumentParser(description="TEDG-RL V4 è®­ç»ƒè„šæœ¬ - Cross-Attention MoE")
    parser.add_argument("--exp-name", type=str, default="v4_full", help="å®éªŒåç§°")
    parser.add_argument("--episodes", type=int, default=10000, help="è®­ç»ƒepisodesæ•°")
    parser.add_argument("--max-steps", type=int, default=2000, help="æ¯episodeæœ€å¤§æ­¥æ•°")
    parser.add_argument("--no-mask", action="store_true", help="ç¦ç”¨åŠ¨ä½œæ©ç ")
    parser.add_argument("--resume", type=str, default=None, help="æ¢å¤è®­ç»ƒçš„checkpointè·¯å¾„")
    parser.add_argument("--freeze-gat", action="store_true", help="å†»ç»“GATå‚æ•°")
    parser.add_argument("--num-experts", type=int, default=4, help="ä¸“å®¶æ•°é‡")
    args = parser.parse_args()
    
    print(f"\n{'='*70}")
    print(f"TEDG-RL V4 è®­ç»ƒå¯åŠ¨ - Cross-Attention Guided Hierarchical MoE")
    print(f"{'='*70}")
    print(f"å®éªŒåç§°: {args.exp_name}")
    print(f"è®­ç»ƒé…ç½®: {args.episodes} episodes, {args.max_steps} steps/episode")
    print(f"ä¸“å®¶æ•°é‡: {args.num_experts}")
    print(f"å†»ç»“GAT: {args.freeze_gat}")
    print(f"\nğŸ†• V4æ–°ç‰¹æ€§:")
    print(f"  - Cross-Attentionèåˆ (æ›¿ä»£V3çš„concat)")
    print(f"  - Sparse Attention Gate (ç¨€ç–æ³¨æ„åŠ›)")
    print(f"  - Context Vector (256ç»´ç´§å‡‘è¡¨ç¤º)")
    
    # è®¾å¤‡æ£€æµ‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nä½¿ç”¨è®¾å¤‡: {device}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(f"ablation_v4/results/{args.exp_name}")
    output_dir.mkdir(parents=True, exist_ok=True)
    (output_dir / "checkpoints").mkdir(exist_ok=True)
    (output_dir / "logs").mkdir(exist_ok=True)
    
    # åŠ è½½è¶…å›¾
    print(f"\n[åŠ è½½è¶…å›¾æ•°æ®]")
    from src.core.state_constructor import StateConstructor
    from src.core.hypergraph_matcher import HypergraphMatcher
    
    state_constructor = StateConstructor("data/hypergraph/hypergraph_complete_real.json")
    matcher = HypergraphMatcher(
        state_constructor.hypergraph,
        weights=(0.35, 0.35, 0.2, 0.1),
        tau=200.0
    )
    print(f"âœ“ è¶…å›¾åŠ è½½å®Œæˆ: {len(matcher.edges)} æ¡è¶…è¾¹")
    
    # åˆ›å»ºç¯å¢ƒ
    print(f"\n[åˆ›å»ºNetHackç¯å¢ƒ]")
    import gymnasium as gym
    try:
        env = gym.make("NetHackScore-v0")
    except:
        env = gym.make("NetHack-v0")
    print(f"âœ“ åŠ¨ä½œç©ºé—´: {env.action_space.n}ä¸ªåŠ¨ä½œ")
    
    # åˆ›å»ºV4ç½‘ç»œ
    policy_net = create_v4_policy(device, args)
    
    # åŠ è½½è¶…å›¾ç»“æ„ä»¥è·å–operator_namesï¼ˆç”¨äºManagerçº¦æŸï¼‰
    print(f"\n[åŠ è½½è¶…å›¾ç»“æ„ç”¨äºManagerçº¦æŸ]")
    import json
    with open("data/hypergraph/hypergraph_gat_structure.json", 'r') as f:
        hypergraph_structure = json.load(f)
    operator_names = [node['label'] for node in hypergraph_structure['nodes'] if node['type'] == 'operator']
    print(f"âœ“ æå–äº† {len(operator_names)} ä¸ªOperatorèŠ‚ç‚¹")
    
    # åˆ›å»ºä¼˜åŒ–å™¨
    import torch.optim as optim
    optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)
    
    # å­¦ä¹ ç‡è°ƒåº¦å™¨
    lr_scheduler = get_lr_scheduler(optimizer, warmup_steps=1000, max_steps=args.episodes*args.max_steps)
    
    # åˆ›å»ºè®­ç»ƒå™¨ (æ²¿ç”¨V3é…ç½®)
    from src.core.ppo_trainer import PPOTrainer
    trainer = PPOTrainer(
        policy_net=policy_net,
        learning_rate=1e-4,
        clip_ratio=0.15,
        gamma=0.995,
        gae_lambda=0.97,
        ppo_epochs=4,
        batch_size=256,
        device=device,
        alpha_entropy_coef=0.05,
    )
    
    # åˆå§‹åŒ–ç›‘æ§å™¨
    monitor = TrainingMonitor(log_interval=50)
    nan_detector = NaNDetector(policy_net)
    reward_normalizer = RewardNormalizer(clip_range=10.0)
    
    # åŠ¨ä½œæ©ç 
    from src.core.action_masking import ActionMasker
    action_masker = ActionMasker(state_constructor.hypergraph, num_actions=23)
    
    # è®­ç»ƒç»Ÿè®¡
    episode_rewards = []
    episode_lengths = []
    episode_scores = []
    best_reward = float("-inf")
    best_score = 0
    start_episode = 0
    
    # æ¢å¤checkpoint
    if args.resume and os.path.exists(args.resume):
        print(f"\n[æ¢å¤checkpoint: {args.resume}]")
        checkpoint = torch.load(args.resume, map_location=device, weights_only=False)
        policy_net.load_state_dict(checkpoint["policy_net"])
        optimizer.load_state_dict(checkpoint["optimizer"])
        start_episode = checkpoint.get("episode", 0) + 1
        best_reward = checkpoint.get("best_reward", float("-inf"))
        best_score = checkpoint.get("best_score", 0)
        print(f"âœ“ ä»Episode {start_episode}ç»§ç»­, Best Score: {best_score}")
    
    print(f"\n{'='*70}")
    print(f"å¼€å§‹è®­ç»ƒ")
    print(f"{'='*70}\n")
    
    import time
    start_time = time.time()
    global_step = 0
    
    # ========================================================================
    # ä¸»è®­ç»ƒå¾ªç¯ (å®Œæ•´æ²¿ç”¨V3é€»è¾‘)
    # ========================================================================
    
    import nle.nethack as nh
    
    for episode in range(start_episode, args.episodes):
        # è·å–å½“å‰é˜¶æ®µé…ç½®
        config = get_training_config(episode)
        
        # æ›´æ–°ç½‘ç»œé…ç½®
        policy_net.use_sparsemax = config['use_sparsemax']
        
        # æ›´æ–°å­¦ä¹ ç‡
        for param_group in optimizer.param_groups:
            param_group['lr'] = config['learning_rate']
        
        # æ‰“å°é˜¶æ®µä¿¡æ¯
        if episode in [0, 1000, 3000]:
            print(f"\n{'='*70}")
            print(f"è¿›å…¥ {config['phase'].upper()} é˜¶æ®µ (Episode {episode})")
            print(f"  - è·¯ç”±æ–¹å¼: {'Sparsemax' if config['use_sparsemax'] else 'Softmax'}")
            print(f"  - å­¦ä¹ ç‡: {config['learning_rate']}")
            print(f"  - è´Ÿè½½å‡è¡¡ç³»æ•°: {config['load_balance_coef']}")
            print(f"{'='*70}\n")
        
        # é‡ç½®ç¯å¢ƒ
        obs, info = env.reset()
        state, atoms = extract_state_from_obs(obs, state_constructor, matcher, t_now=0)
        
        done = False
        truncated = False
        total_reward = 0
        steps = 0
        
        # Episodeå†…çš„ç»Ÿè®¡
        episode_alphas = []
        episode_expert_logits = []
        episode_alignment_losses = []
        episode_semantic_losses = []
        episode_temporal_losses = []
        episode_overlap_losses = []
        
        # æ—¶é—´ä¸€è‡´æ€§è¿½è¸ª
        last_alpha = None
        
        # ====================================================================
        # Episodeå¾ªç¯
        # ====================================================================
        
        while not (done or truncated) and steps < args.max_steps:
            # è·å–åŠ¨ä½œ
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
                
                # V4å‰å‘ä¼ æ’­ (ä½¿ç”¨æ–°çš„forwardæ¥å£)
                outputs = policy_net.forward(
                    state_tensor,
                    atoms=atoms,
                    return_expert_logits=True
                )
                
                logits = outputs['policy_logits']
                alpha = outputs['alpha']
                value = outputs['value']
                
                # è®°å½•ç»Ÿè®¡ä¿¡æ¯
                episode_alphas.append(alpha.cpu())
                if 'expert_logits' in outputs and outputs['expert_logits'] is not None:
                    episode_expert_logits.append(outputs['expert_logits'].cpu())
                
                # åº”ç”¨åŠ¨ä½œæ©ç 
                action_mask = None
                if not args.no_mask:
                    action_mask = action_masker.get_action_mask(
                        atoms["pre_nodes"],
                        atoms["scene_atoms"],
                        0.5  # confidence
                    )
                    mask_t = torch.as_tensor(action_mask, device=logits.device, dtype=torch.bool)
                    masked_logits = logits.masked_fill(~mask_t, float("-inf"))
                    
                    # å…œåº•: é¿å…å…¨-inf
                    if not torch.isfinite(masked_logits).any():
                        masked_logits = logits
                        action_mask = np.ones(23, dtype=bool)
                    
                    logits = masked_logits
                
                # æ•°å€¼ç¨³å®šæ€§
                logits = torch.nan_to_num(logits, nan=0.0, posinf=20.0, neginf=-20.0).clamp(-20.0, 20.0)
                
                # é‡‡æ ·åŠ¨ä½œ
                dist = torch.distributions.Categorical(logits=logits)
                action = dist.sample()
                log_prob = dist.log_prob(action)
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, truncated, info = env.step(action.item())
            
            # å¥–åŠ±å½’ä¸€åŒ–
            reward_normalizer.update(reward)
            normalized_reward = reward_normalizer.normalize(reward)
            
            total_reward += reward
            steps += 1
            global_step += 1
            
            # æå–ä¸‹ä¸€çŠ¶æ€
            next_state, next_atoms = extract_state_from_obs(obs, state_constructor, matcher, t_now=steps)
            
            # å­˜å‚¨ç»éªŒ
            trainer.buffer.add(
                state=state,
                action=action.item(),
                reward=normalized_reward,
                next_state=next_state,
                done=done or truncated,
                log_prob=log_prob.item(),
                action_mask=action_mask,
            )
            
            state = next_state
            atoms = next_atoms
            
            # æ›´æ–°ç½‘ç»œ
            if len(trainer.buffer) >= trainer.batch_size:
                # ä¿å­˜checkpoint (ç”¨äºNaNå›æ»š)
                nan_detector.save_checkpoint()
                
                # é‡‡æ ·æ‰¹æ¬¡
                batch = trainer.buffer.sample_batch(trainer.batch_size)
                states = batch['states'].to(device)
                actions = batch['actions'].to(device)
                rewards = batch['rewards'].to(device)
                dones = batch['dones'].to(device)
                old_log_probs = batch['old_log_probs'].to(device)
                
                # è®¡ç®—GAEä¼˜åŠ¿
                with torch.no_grad():
                    old_outputs = policy_net.forward(states, atoms=atoms)
                    old_values = old_outputs['value'].squeeze(-1)
                
                advantages, returns = trainer.compute_gae_advantages(rewards, old_values, dones)
                
                # å½’ä¸€åŒ–ä¼˜åŠ¿
                adv_mean = advantages.mean()
                adv_std = advantages.cpu().std().to(advantages.device) if advantages.is_cuda or str(advantages.device).startswith('musa') else advantages.std()
                advantages = (advantages - adv_mean) / (adv_std + 1e-8)
                
                # PPOæ›´æ–°å¾ªç¯
                for ppo_epoch in range(trainer.ppo_epochs):
                    # å‰å‘ä¼ æ’­
                    outputs = policy_net.forward(
                        states,
                        atoms=atoms,
                        return_expert_logits=True
                    )
                    
                    logits = outputs['policy_logits']
                    alpha = outputs['alpha']
                    values = outputs['value'].squeeze(-1)
                    
                    # è®¡ç®—æ–°çš„å¯¹æ•°æ¦‚ç‡
                    dist = torch.distributions.Categorical(logits=logits)
                    new_log_probs = dist.log_prob(actions)
                    entropy = dist.entropy().mean()
                    
                    # è®¡ç®—Î±ç†µ
                    alpha_entropy = -(alpha * torch.log(alpha + 1e-8)).sum(dim=-1).mean()
                    
                    # PPOæ¯”ç‡
                    ratio = torch.exp(new_log_probs - old_log_probs)
                    
                    # ActoræŸå¤±
                    surr1 = ratio * advantages
                    surr2 = torch.clamp(ratio, 1 - trainer.clip_ratio, 1 + trainer.clip_ratio) * advantages
                    actor_loss = -torch.min(surr1, surr2).mean()
                    
                    # CriticæŸå¤±
                    critic_loss = F.mse_loss(values, returns)
                    
                    # è¾…åŠ©æŸå¤±
                    lb_loss = load_balance_loss(alpha, num_experts=args.num_experts)
                    
                    div_loss = torch.tensor(0.0, device=device)
                    if 'expert_logits' in outputs and outputs['expert_logits'] is not None:
                        div_loss = expert_diversity_loss(outputs['expert_logits'])
                    
                    # ===== Managerå†…å±‚çº¦æŸ =====
                    alignment_loss = torch.tensor(0.0, device=device)
                    semantic_loss = torch.tensor(0.0, device=device)
                    
                    if 'operator_scores' in outputs and outputs['operator_scores'] is not None and 'expert_logits' in outputs and outputs['expert_logits'] is not None:
                        # 1. è¶…å›¾-è·¯ç”±å¯¹é½æŸå¤±
                        alignment_loss = hypergraph_alignment_loss(
                            outputs['operator_scores'],
                            alpha,
                            operator_names,
                            temperature=config.get('alignment_temperature', 1.0)
                        )
                        
                        # 2. å¢å¼ºçš„è¯­ä¹‰æ­£äº¤æŸå¤±
                        semantic_loss = enhanced_semantic_orthogonality_loss(
                            outputs['expert_logits']
                        )
                    
                    # ===== é«˜çº§æœºåˆ¶ =====
                    # 3. æ—¶é—´ä¸€è‡´æ€§æŸå¤±
                    temporal_loss = torch.tensor(0.0, device=device)
                    if last_alpha is not None and config.get('temporal_coef', 0.0) > 0:
                        temporal_loss = F.mse_loss(alpha, last_alpha)
                    
                    # 4. ä¸“å®¶é‡å æƒ©ç½š
                    overlap_loss = torch.tensor(0.0, device=device)
                    if 'expert_logits' in outputs and outputs['expert_logits'] is not None and config.get('overlap_coef', 0.0) > 0:
                        overlap_loss = expert_overlap_penalty(alpha, outputs['expert_logits'])
                    
                    # æ€»æŸå¤±ï¼ˆåŒ…å«æ‰€æœ‰çº¦æŸï¼‰
                    total_loss = (
                        actor_loss +
                        0.5 * critic_loss -
                        config['entropy_coef'] * entropy +
                        config.get('alpha_entropy_sign', -1) * config['alpha_entropy_coef'] * alpha_entropy +
                        config['load_balance_coef'] * lb_loss +
                        config['diversity_coef'] * div_loss +
                        config.get('alignment_coef', 0.1) * alignment_loss +
                        config.get('semantic_coef', 0.05) * semantic_loss +
                        config.get('temporal_coef', 0.0) * temporal_loss +
                        config.get('overlap_coef', 0.0) * overlap_loss
                    )
                    
                    # NaNæ£€æµ‹
                    if nan_detector.check_and_rollback(total_loss):
                        break
                    
                    # åå‘ä¼ æ’­
                    optimizer.zero_grad()
                    total_loss.backward()
                    
                    # æ¢¯åº¦è£å‰ª
                    grad_norm = torch.nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)
                    
                    # æ›´æ–°
                    optimizer.step()
                    lr_scheduler.step()
                
                # è®°å½•Managerçº¦æŸæŸå¤±
                if alignment_loss.item() > 0:
                    episode_alignment_losses.append(alignment_loss.item())
                    episode_semantic_losses.append(semantic_loss.item())
                
                # è®°å½•é«˜çº§æœºåˆ¶æŸå¤±
                if temporal_loss.item() > 0:
                    episode_temporal_losses.append(temporal_loss.item())
                if overlap_loss.item() > 0:
                    episode_overlap_losses.append(overlap_loss.item())
                
                # æ›´æ–°last_alpha
                last_alpha = alpha.detach()
                
                # æ¸…ç©ºç¼“å†²åŒº
                trainer.buffer.clear()
        
        # ====================================================================
        # Episodeç»“æŸç»Ÿè®¡
        # ====================================================================
        
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        
        final_score = obs.get("blstats", [0] * nh.NLE_BLSTATS_SIZE)[nh.NLE_BL_SCORE]
        episode_scores.append(final_score)
        
        # è®¡ç®—ä¸“å®¶ç»Ÿè®¡
        if episode_alphas:
            alpha_tensor = torch.stack(episode_alphas, dim=0)
            mean_alpha = alpha_tensor.mean(dim=0).numpy()
            alpha_entropy_val = -(mean_alpha * np.log(mean_alpha + 1e-8)).sum()
            expert_usage_variance = alpha_tensor.var(dim=0).mean().item()
        else:
            alpha_entropy_val = 0.0
            expert_usage_variance = 0.0
        
        # è®°å½•ç›‘æ§æŒ‡æ ‡
        monitor.log(episode, {
            'episode_score': final_score,
            'episode_reward': total_reward,
            'episode_length': steps,
            'alpha_entropy': alpha_entropy_val,
            'expert_usage_variance': expert_usage_variance,
            'gradient_norm': grad_norm.item() if 'grad_norm' in locals() else 0.0,
        })
        
        # æ›´æ–°æœ€ä½³è®°å½•
        if total_reward > best_reward:
            best_reward = total_reward
            torch.save({
                "policy_net": policy_net.state_dict(),
                "optimizer": optimizer.state_dict(),
                "episode": episode,
                "best_reward": best_reward,
                "best_score": final_score,
                "config": vars(args)
            }, output_dir / "checkpoints" / "best_model.pth")
        
        if final_score > best_score:
            best_score = final_score
        
        # å®šæœŸä¿å­˜checkpoint
        if (episode + 1) % 100 == 0:
            torch.save({
                "policy_net": policy_net.state_dict(),
                "optimizer": optimizer.state_dict(),
                "episode": episode,
                "reward": total_reward,
                "score": final_score,
                "config": vars(args)
            }, output_dir / "checkpoints" / f"model_{episode+1:05d}.pth")
        
        # æ‰“å°è¿›åº¦
        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(episode_rewards[-10:])
            avg_score = np.mean(episode_scores[-10:])
            print(f"Episode {episode+1}/{args.episodes} | "
                  f"Score: {final_score} | "
                  f"Reward: {total_reward:.2f} | "
                  f"Steps: {steps} | "
                  f"Î±_entropy: {alpha_entropy_val:.3f} | "
                  f"Phase: {config['phase']}")
            
            # æ‰“å°Managerçº¦æŸçš„loss
            if episode_alignment_losses:
                avg_alignment = np.mean(episode_alignment_losses)
                avg_semantic = np.mean(episode_semantic_losses)
                print(f"  â†’ Manager Constraints: "
                      f"Alignment={avg_alignment:.4f}, "
                      f"Semantic={avg_semantic:.4f}")
            
            # æ‰“å°é«˜çº§æœºåˆ¶çš„loss
            if episode_temporal_losses or episode_overlap_losses:
                losses_str = "  â†’ Advanced Mechanisms: "
                if episode_temporal_losses:
                    avg_temporal = np.mean(episode_temporal_losses)
                    losses_str += f"Temporal={avg_temporal:.4f}, "
                if episode_overlap_losses:
                    avg_overlap = np.mean(episode_overlap_losses)
                    losses_str += f"Overlap={avg_overlap:.4f}"
                print(losses_str)
    
    
    # ========================================================================
    # è®­ç»ƒç»“æŸ
    # ========================================================================
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save({
        "policy_net": policy_net.state_dict(),
        "optimizer": optimizer.state_dict(),
        "episode": args.episodes,
        "best_reward": best_reward,
        "best_score": best_score,
        "config": vars(args)
    }, output_dir / "checkpoints" / "model_final.pth")
    
    # ä¿å­˜è®­ç»ƒæ—¥å¿—
    training_log = {
        "episode_rewards": [float(r) for r in episode_rewards],
        "episode_lengths": [int(l) for l in episode_lengths],
        "episode_scores": [int(s) for s in episode_scores],
        "best_reward": float(best_reward),
        "best_score": int(best_score),
        "config": vars(args),
        "monitor_metrics": {k: [float(v) for v in vals] for k, vals in monitor.metrics.items()},
    }
    
    with open(output_dir / "logs" / "training_log.json", "w") as f:
        json.dump(training_log, f, indent=2)
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    elapsed_time = time.time() - start_time
    print(f"\n{'='*70}")
    print(f"è®­ç»ƒå®Œæˆ")
    print(f"{'='*70}")
    print(f"æ€»è€—æ—¶: {elapsed_time/3600:.2f} å°æ—¶")
    print(f"æœ€ä½³å¥–åŠ±: {best_reward:.2f}")
    print(f"æœ€ä½³åˆ†æ•°: {best_score}")
    print(f"å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f}")
    print(f"å¹³å‡åˆ†æ•°: {np.mean(episode_scores):.1f}")
    print(f"ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"{'='*70}\n")


if __name__ == '__main__':
    main()
