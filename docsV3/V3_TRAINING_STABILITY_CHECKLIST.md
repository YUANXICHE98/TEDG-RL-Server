# V3 è®­ç»ƒç¨³å®šæ€§æ£€æŸ¥æ¸…å•

> **ç‰ˆæœ¬**: V3.0 - GAT-Guided Hierarchical MoE  
> **åˆ›å»ºæ—¥æœŸ**: 2025-01-05  
> **ç›®çš„**: ç¡®ä¿V3è®­ç»ƒç¨³å®šã€æ”¶æ•›ã€æ— å¡Œç¼©  
> **çŠ¶æ€**: è®¾è®¡é˜¶æ®µ â†’ å®ç°å‰å¿…è¯»

---

## ä¸€ã€æ¦‚è¿°

V3å¼•å…¥äº†GATå’ŒSparsemaxè·¯ç”±ï¼Œç›¸æ¯”V1/V2æœ‰æ›´å¤šæ½œåœ¨çš„è®­ç»ƒä¸ç¨³å®šå› ç´ ï¼š
- **GATè¿‡å¹³æ»‘**: å¤šå±‚GATå¯èƒ½å¯¼è‡´èŠ‚ç‚¹ç‰¹å¾è¶‹åŒ
- **ä¸“å®¶å¡Œç¼©**: è·¯ç”±å™¨å¯èƒ½åªé€‰æ‹©1-2ä¸ªä¸“å®¶ï¼Œå…¶ä»–ä¸“å®¶é€€åŒ–
- **æ¢¯åº¦çˆ†ç‚¸**: GAT + MoEçš„æ·±å±‚ç½‘ç»œå®¹æ˜“æ¢¯åº¦ä¸ç¨³å®š
- **å¥–åŠ±ç¨€ç–**: NetHackçš„é•¿ç¨‹ç¨€ç–å¥–åŠ±é—®é¢˜
- **æ•°å€¼ä¸ç¨³å®š**: NaN/Infå¯¼è‡´è®­ç»ƒå´©æºƒ

æœ¬æ–‡æ¡£æä¾›**å®Œæ•´çš„ç¨³å®šæ€§æªæ–½æ¸…å•**ï¼Œæ¶µç›–ï¼š
1. ç½‘ç»œæ¶æ„è®¾è®¡
2. è®­ç»ƒè¶…å‚æ•°
3. è¾…åŠ©æŸå¤±å’Œæ­£åˆ™åŒ–
4. æ•°å€¼ç¨³å®šæ€§æŠ€å·§
5. ç›‘æ§å’Œè¯Šæ–­
6. é™çº§æ–¹æ¡ˆ

---

## äºŒã€ç½‘ç»œæ¶æ„ç¨³å®šæ€§æªæ–½

### 2.1 GATå±‚è®¾è®¡

#### âœ… å·²å®ç°
- **2å±‚GAT**: é™åˆ¶å±‚æ•°ï¼Œé¿å…è¿‡å¹³æ»‘
- **æ®‹å·®è¿æ¥**: `x2 = x1 + GAT(x1)` ä¿æŒæ¢¯åº¦æµ
- **LayerNorm**: æ¯å±‚åå½’ä¸€åŒ–ï¼Œç¨³å®šæ¿€æ´»å€¼
- **å¤šå¤´æ³¨æ„åŠ›**: 4ä¸ªå¤´ï¼Œå¢åŠ è¡¨è¾¾èƒ½åŠ›
- **Dropout**: 0.1ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ

#### ğŸ”§ å¾…å®ç°
- **æ³¨æ„åŠ›æ¸©åº¦**: æ·»åŠ å¯å­¦ä¹ æ¸©åº¦å‚æ•°ï¼Œé˜²æ­¢æ³¨æ„åŠ›è¿‡äºå°–é”
- **è¾¹Dropout**: è®­ç»ƒæ—¶éšæœºä¸¢å¼ƒéƒ¨åˆ†è¾¹ï¼Œå¢å¼ºé²æ£’æ€§
- **èŠ‚ç‚¹ç‰¹å¾è£å‰ª**: é™åˆ¶èŠ‚ç‚¹åµŒå…¥çš„L2èŒƒæ•°ï¼Œé˜²æ­¢çˆ†ç‚¸


```python
# ç¤ºä¾‹: æ·»åŠ æ³¨æ„åŠ›æ¸©åº¦
class GATConvWithTemp(GATConv):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.temp = nn.Parameter(torch.ones(1))  # å¯å­¦ä¹ æ¸©åº¦
    
    def forward(self, x, edge_index):
        alpha = self.attention(x, edge_index) / self.temp  # æ¸©åº¦ç¼©æ”¾
        return super().forward(x, edge_index, alpha=alpha)
```

### 2.2 è·¯ç”±å™¨è®¾è®¡

#### âœ… å·²å®ç°
- **Sparsemaxæ¿€æ´»**: è‡ªåŠ¨ç¨€ç–åŒ–ï¼Œé¿å…å¹³å‡ä¸»ä¹‰
- **3å±‚MLP**: 512â†’128â†’64â†’4ï¼Œé€æ­¥é™ç»´
- **LayerNorm**: æ¯å±‚åå½’ä¸€åŒ–
- **å°å¢ç›Šåˆå§‹åŒ–**: æœ€åä¸€å±‚æƒé‡ç”¨0.01å¢ç›Š

#### ğŸ”§ å¾…å®ç°
- **æ¸©åº¦é€€ç«**: Sparsemaxæ¸©åº¦ä»1.0é€æ¸é™åˆ°0.5
- **Warmupé˜¶æ®µ**: å‰1000 episodesä½¿ç”¨Softmaxï¼Œé¿å…è¿‡æ—©å¡Œç¼©
- **è´Ÿè½½å‡è¡¡æŸå¤±**: é¼“åŠ±ä¸“å®¶ä½¿ç”¨å‡è¡¡

```python
# ç¤ºä¾‹: æ¸©åº¦é€€ç«
def get_sparsemax_temp(episode, warmup=1000, max_episodes=10000):
    if episode < warmup:
        return 1.0  # Warmupé˜¶æ®µ: Softmax
    else:
        # çº¿æ€§é€€ç«: 1.0 â†’ 0.5
        progress = (episode - warmup) / (max_episodes - warmup)
        return 1.0 - 0.5 * progress
```


### 2.3 ä¸“å®¶ç½‘ç»œè®¾è®¡

#### âœ… å·²å®ç°
- **ç‹¬ç«‹MLP**: æ¯ä¸ªä¸“å®¶æ˜¯ç‹¬ç«‹çš„2å±‚MLP
- **LayerNorm**: ç¨³å®šæ¿€æ´»å€¼
- **å°å¢ç›Šåˆå§‹åŒ–**: è¾“å‡ºå±‚0.01å¢ç›Šï¼Œé˜²æ­¢åˆå§‹logitsè¿‡å¤§

#### ğŸ”§ å¾…å®ç°
- **ä¸“å®¶æ­£åˆ™åŒ–**: L2æ­£åˆ™åŒ–ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
- **ä¸“å®¶å¤šæ ·æ€§æŸå¤±**: é¼“åŠ±ä¸“å®¶å­¦åˆ°ä¸åŒç­–ç•¥

```python
# ç¤ºä¾‹: ä¸“å®¶å¤šæ ·æ€§æŸå¤±
def expert_diversity_loss(expert_logits):
    """
    é¼“åŠ±ä¸“å®¶è¾“å‡ºä¸åŒçš„åŠ¨ä½œåˆ†å¸ƒ
    
    Args:
        expert_logits: (batch, num_experts, action_dim)
    
    Returns:
        diversity_loss: æ ‡é‡
    """
    # è®¡ç®—ä¸“å®¶é—´çš„ä½™å¼¦ç›¸ä¼¼åº¦
    num_experts = expert_logits.size(1)
    diversity_loss = 0.0
    
    for i in range(num_experts):
        for j in range(i+1, num_experts):
            cos_sim = F.cosine_similarity(
                expert_logits[:, i, :], 
                expert_logits[:, j, :], 
                dim=-1
            ).mean()
            diversity_loss += cos_sim
    
    # å½’ä¸€åŒ–
    diversity_loss /= (num_experts * (num_experts - 1) / 2)
    
    return diversity_loss
```


### 2.4 Criticç½‘ç»œè®¾è®¡

#### âœ… å·²å®ç°
- **åŒæµè¾“å…¥**: h_vis + h_logic (512ç»´)
- **3å±‚MLP**: 512â†’256â†’128â†’1
- **LayerNorm**: ç¨³å®šè®­ç»ƒ

#### ğŸ”§ å¾…å®ç°
- **ä»·å€¼è£å‰ª**: é™åˆ¶ä»·å€¼ä¼°è®¡èŒƒå›´ï¼Œé˜²æ­¢çˆ†ç‚¸
- **åŒCritic**: ä½¿ç”¨ä¸¤ä¸ªCriticå–æœ€å°å€¼ï¼Œå‡å°‘è¿‡ä¼°è®¡

```python
# ç¤ºä¾‹: ä»·å€¼è£å‰ª
def forward_critic(self, z):
    value = self.critic(z)
    value = torch.clamp(value, -1000, 1000)  # è£å‰ªåˆ°åˆç†èŒƒå›´
    return value
```

---

## ä¸‰ã€è®­ç»ƒè¶…å‚æ•°è®¾ç½®

### 3.1 å­¦ä¹ ç‡

| å‚æ•° | V1/V2 | V3 (æ¨è) | ç†ç”± |
|------|-------|-----------|------|
| **learning_rate** | 3e-4 | **1e-4** | GATéœ€è¦æ›´å°å­¦ä¹ ç‡ |
| **lr_scheduler** | æ—  | **CosineAnnealing** | åæœŸç²¾ç»†è°ƒæ•´ |
| **warmup_steps** | 0 | **1000** | é¿å…åˆæœŸæ¢¯åº¦çˆ†ç‚¸ |

```python
# ç¤ºä¾‹: å­¦ä¹ ç‡Warmup + CosineAnnealing
from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR

def get_lr_scheduler(optimizer, warmup_steps=1000, max_steps=100000):
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps  # çº¿æ€§warmup
        else:
            return 1.0
    
    warmup_scheduler = LambdaLR(optimizer, lr_lambda)
    cosine_scheduler = CosineAnnealingLR(optimizer, T_max=max_steps-warmup_steps)
    
    return warmup_scheduler, cosine_scheduler
```


### 3.2 PPOè¶…å‚æ•°

| å‚æ•° | V1/V2 | V3 (æ¨è) | ç†ç”± |
|------|-------|-----------|------|
| **clip_ratio** | 0.2 | **0.15** | æ›´ä¿å®ˆçš„ç­–ç•¥æ›´æ–° |
| **ppo_epochs** | 3 | **4** | æ›´å……åˆ†çš„ç­–ç•¥ä¼˜åŒ– |
| **batch_size** | 128 | **256** | æ›´ç¨³å®šçš„æ¢¯åº¦ä¼°è®¡ |
| **gamma** | 0.99 | **0.995** | NetHackéœ€è¦æ›´é•¿è§†é‡ |
| **gae_lambda** | 0.95 | **0.97** | æ›´å¹³æ»‘çš„ä¼˜åŠ¿ä¼°è®¡ |

### 3.3 æ­£åˆ™åŒ–ç³»æ•°

| å‚æ•° | V1/V2 | V3 (æ¨è) | ç†ç”± |
|------|-------|-----------|------|
| **entropy_coef** | 0.05 | **0.01** | Sparsemaxå·²ç¨€ç–ï¼Œé™ä½ç†µæ­£åˆ™ |
| **alpha_entropy_coef** | 0.1 | **0.05** | é¼“åŠ±ä¸“å®¶å‡è¡¡ä½¿ç”¨ |
| **diversity_coef** | 0 | **0.01** | é¼“åŠ±ä¸“å®¶å¤šæ ·æ€§ |
| **load_balance_coef** | 0 | **0.01** | é˜²æ­¢ä¸“å®¶å¡Œç¼© |

---

## å››ã€è¾…åŠ©æŸå¤±å’Œæ­£åˆ™åŒ–

### 4.1 ä¸“å®¶è´Ÿè½½å‡è¡¡æŸå¤±

**ç›®çš„**: é˜²æ­¢è·¯ç”±å™¨åªé€‰æ‹©1-2ä¸ªä¸“å®¶ï¼Œå…¶ä»–ä¸“å®¶é€€åŒ–

```python
def load_balance_loss(alpha, num_experts=4):
    """
    è´Ÿè½½å‡è¡¡æŸå¤±: é¼“åŠ±æ¯ä¸ªä¸“å®¶è¢«å‡åŒ€ä½¿ç”¨
    
    Args:
        alpha: (batch, num_experts) ä¸“å®¶æƒé‡
    
    Returns:
        loss: æ ‡é‡
    """
    # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„å¹³å‡ä½¿ç”¨ç‡
    expert_usage = alpha.mean(dim=0)  # (num_experts,)
    
    # ç†æƒ³æƒ…å†µ: æ¯ä¸ªä¸“å®¶ä½¿ç”¨ç‡ = 1/num_experts
    target_usage = torch.ones_like(expert_usage) / num_experts
    
    # L2æŸå¤±
    loss = F.mse_loss(expert_usage, target_usage)
    
    return loss
```


### 4.2 ä¸“å®¶å¤šæ ·æ€§æŸå¤±

**ç›®çš„**: é¼“åŠ±ä¸åŒä¸“å®¶å­¦åˆ°ä¸åŒçš„ç­–ç•¥

```python
def expert_diversity_loss(expert_logits):
    """
    ä¸“å®¶å¤šæ ·æ€§æŸå¤±: æœ€å°åŒ–ä¸“å®¶é—´çš„ç›¸ä¼¼åº¦
    
    Args:
        expert_logits: (batch, num_experts, action_dim)
    
    Returns:
        loss: æ ‡é‡
    """
    num_experts = expert_logits.size(1)
    
    # è®¡ç®—ä¸“å®¶è¾“å‡ºçš„åæ–¹å·®çŸ©é˜µ
    # å±•å¹³: (batch*num_experts, action_dim)
    flat_logits = expert_logits.view(-1, expert_logits.size(-1))
    
    # ä¸­å¿ƒåŒ–
    mean_logits = flat_logits.mean(dim=0, keepdim=True)
    centered = flat_logits - mean_logits
    
    # åæ–¹å·®çŸ©é˜µ
    cov = torch.mm(centered.t(), centered) / centered.size(0)
    
    # å¯¹è§’çº¿å¤–çš„å…ƒç´  (ä¸“å®¶é—´ç›¸ä¼¼åº¦)
    off_diag = cov - torch.diag(torch.diag(cov))
    
    # æœ€å°åŒ–ç›¸ä¼¼åº¦
    loss = off_diag.abs().mean()
    
    return loss
```

### 4.3 GATæ³¨æ„åŠ›æ­£åˆ™åŒ–

**ç›®çš„**: é˜²æ­¢GATæ³¨æ„åŠ›è¿‡äºé›†ä¸­æˆ–è¿‡äºåˆ†æ•£

```python
def attention_regularization(attention_weights, target_sparsity=0.3):
    """
    æ³¨æ„åŠ›æ­£åˆ™åŒ–: é¼“åŠ±é€‚åº¦ç¨€ç–çš„æ³¨æ„åŠ›
    
    Args:
        attention_weights: (num_edges,) GATæ³¨æ„åŠ›æƒé‡
        target_sparsity: ç›®æ ‡ç¨€ç–åº¦ (0-1)
    
    Returns:
        loss: æ ‡é‡
    """
    # è®¡ç®—å®é™…ç¨€ç–åº¦ (Giniç³»æ•°)
    sorted_weights, _ = torch.sort(attention_weights)
    n = len(sorted_weights)
    index = torch.arange(1, n+1, device=sorted_weights.device)
    gini = (2 * (index * sorted_weights).sum()) / (n * sorted_weights.sum()) - (n+1)/n
    
    # L2æŸå¤±
    loss = (gini - target_sparsity) ** 2
    
    return loss
```


### 4.4 Next-Intent Prediction (è¾…åŠ©ä»»åŠ¡)

**ç›®çš„**: å¼ºè¿«GATå­¦ä¹ å› æœå…³ç³»ï¼Œé¢„æµ‹ä¸‹ä¸€æ­¥å“ªä¸ªOperatorä¼šæ¿€æ´»

```python
def next_intent_prediction_loss(h_logic, next_operator_mask):
    """
    ä¸‹ä¸€æ„å›¾é¢„æµ‹æŸå¤±: é¢„æµ‹ä¸‹ä¸€æ­¥æ¿€æ´»çš„Operator
    
    Args:
        h_logic: (batch, hidden_dim) Intent Vector
        next_operator_mask: (batch, num_operators) ä¸‹ä¸€æ­¥æ¿€æ´»çš„Operator (0/1)
    
    Returns:
        loss: æ ‡é‡
    """
    # é¢„æµ‹å¤´
    predictor = nn.Linear(hidden_dim, num_operators)
    
    # é¢„æµ‹logits
    pred_logits = predictor(h_logic)  # (batch, num_operators)
    
    # äºŒåˆ†ç±»äº¤å‰ç†µ
    loss = F.binary_cross_entropy_with_logits(
        pred_logits, 
        next_operator_mask.float()
    )
    
    return loss
```

**ä½¿ç”¨æ–¹æ³•**:
1. åœ¨è®­ç»ƒå¾ªç¯ä¸­ï¼Œè®°å½•å½“å‰å’Œä¸‹ä¸€æ­¥çš„atoms
2. ä»atomsæ„é€ next_operator_mask
3. å°†æ­¤æŸå¤±åŠ å…¥æ€»æŸå¤±: `total_loss += 0.1 * next_intent_loss`

---

## äº”ã€æ¢¯åº¦å’Œæ•°å€¼ç¨³å®šæ€§

### 5.1 æ¢¯åº¦è£å‰ª

#### âœ… å·²å®ç° (PPO Trainer)
```python
nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)
```

#### ğŸ”§ æ¨èè°ƒæ•´
- **V3æ¨è**: `max_norm=1.0` (æ›´ä¿å®ˆ)
- **ç›‘æ§**: è®°å½•æ¢¯åº¦èŒƒæ•°ï¼Œæ£€æµ‹çˆ†ç‚¸

```python
# ç¤ºä¾‹: ç›‘æ§æ¢¯åº¦èŒƒæ•°
def log_gradient_norms(model):
    total_norm = 0.0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    total_norm = total_norm ** 0.5
    
    print(f"Gradient norm: {total_norm:.4f}")
    
    if total_norm > 10.0:
        print(f"âš ï¸ æ¢¯åº¦çˆ†ç‚¸è­¦å‘Š: {total_norm:.4f}")
```


### 5.2 NaN/Infå¤„ç†

#### âœ… å·²å®ç°
```python
# åœ¨ networks_v3_gat_moe.py
logits = torch.nan_to_num(logits, nan=0.0, posinf=20.0, neginf=-20.0).clamp(-20.0, 20.0)
value = torch.nan_to_num(value, nan=0.0, posinf=1e3, neginf=-1e3)
```

#### ğŸ”§ é¢å¤–æªæ–½
- **æ£€æŸ¥ç‚¹**: æ¯æ¬¡å‰å‘ä¼ æ’­åæ£€æŸ¥NaN
- **è‡ªåŠ¨å›æ»š**: æ£€æµ‹åˆ°NaNæ—¶å›æ»šåˆ°ä¸Šä¸€ä¸ªcheckpoint

```python
# ç¤ºä¾‹: NaNæ£€æµ‹å’Œå›æ»š
class NaNDetector:
    def __init__(self, model):
        self.model = model
        self.last_good_state = None
    
    def save_checkpoint(self):
        self.last_good_state = {
            k: v.clone() for k, v in self.model.state_dict().items()
        }
    
    def check_and_rollback(self, loss):
        if torch.isnan(loss) or torch.isinf(loss):
            print("âš ï¸ NaN/Inf detected! Rolling back...")
            if self.last_good_state:
                self.model.load_state_dict(self.last_good_state)
            return True
        return False
```

### 5.3 æƒé‡åˆå§‹åŒ–

#### âœ… å·²å®ç°
- **ä¸“å®¶è¾“å‡ºå±‚**: Orthogonalåˆå§‹åŒ–ï¼Œgain=0.01
- **åµŒå…¥å±‚**: é»˜è®¤åˆå§‹åŒ–

#### ğŸ”§ æ¨è
- **GATå±‚**: Xavieråˆå§‹åŒ–
- **è·¯ç”±å™¨**: Heåˆå§‹åŒ–

```python
# ç¤ºä¾‹: è‡ªå®šä¹‰åˆå§‹åŒ–
def init_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight)
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)
    elif isinstance(m, GATConv):
        nn.init.xavier_uniform_(m.lin.weight)

policy_net.apply(init_weights)
```


---

## å…­ã€å¥–åŠ±å¡‘å½¢å’Œç¨€ç–å¥–åŠ±å¤„ç†

### 6.1 å¥–åŠ±å½’ä¸€åŒ–

**é—®é¢˜**: NetHackå¥–åŠ±èŒƒå›´å¤§ (-1000åˆ°+1000)ï¼Œå¯¼è‡´ä»·å€¼ä¼°è®¡ä¸ç¨³å®š

```python
class RewardNormalizer:
    def __init__(self, clip_range=10.0):
        self.mean = 0.0
        self.std = 1.0
        self.clip_range = clip_range
        self.count = 0
    
    def update(self, reward):
        self.count += 1
        delta = reward - self.mean
        self.mean += delta / self.count
        self.std = np.sqrt((self.std**2 * (self.count-1) + delta**2) / self.count)
    
    def normalize(self, reward):
        normalized = (reward - self.mean) / (self.std + 1e-8)
        return np.clip(normalized, -self.clip_range, self.clip_range)
```

### 6.2 å¥–åŠ±å¡‘å½¢

**V2å·²æœ‰**: 5åˆ†é‡å¥–åŠ± (progress, safety, efficiency, feasibility, exploration)

**V3å¢å¼º**: æ·»åŠ GATç›¸å…³å¥–åŠ±

```python
def compute_v3_reward(env_reward, gat_info, expert_info):
    """
    V3å¥–åŠ±å¡‘å½¢
    
    Args:
        env_reward: ç¯å¢ƒåŸå§‹å¥–åŠ±
        gat_info: GATç›¸å…³ä¿¡æ¯ (operatoræ¿€æ´»ç‡ç­‰)
        expert_info: ä¸“å®¶ç›¸å…³ä¿¡æ¯ (Î±æƒé‡ç­‰)
    
    Returns:
        shaped_reward: å¡‘å½¢åçš„å¥–åŠ±
    """
    # åŸºç¡€å¥–åŠ±
    r_base = env_reward / 1000.0
    
    # GATå¥–åŠ±: é¼“åŠ±æ¿€æ´»æœ‰æ„ä¹‰çš„Operator
    r_gat = 0.01 * gat_info['operator_activation_rate']
    
    # ä¸“å®¶å¥–åŠ±: é¼“åŠ±æ˜ç¡®çš„ä¸“å®¶é€‰æ‹© (é«˜Î±ç†µæƒ©ç½š)
    alpha_entropy = -(expert_info['alpha'] * np.log(expert_info['alpha'] + 1e-8)).sum()
    r_expert = -0.01 * alpha_entropy  # ç†µè¶Šä½è¶Šå¥½
    
    return r_base + r_gat + r_expert
```


### 6.3 å¥–åŠ±è£å‰ª

**ç›®çš„**: é˜²æ­¢æç«¯å¥–åŠ±ç ´åè®­ç»ƒ

```python
def clip_reward(reward, clip_range=10.0):
    return np.clip(reward, -clip_range, clip_range)
```

---

## ä¸ƒã€è®­ç»ƒæµç¨‹å’ŒWarmupæœºåˆ¶

### 7.1 ä¸‰é˜¶æ®µè®­ç»ƒ

| é˜¶æ®µ | Episodes | ç‰¹ç‚¹ | ç›®çš„ |
|------|----------|------|------|
| **Warmup** | 0-1000 | Softmaxè·¯ç”±, é«˜å­¦ä¹ ç‡ | è®©ä¸“å®¶å­¦åˆ°åŸºç¡€ç­–ç•¥ |
| **Transition** | 1000-3000 | æ¸©åº¦é€€ç«, é€æ¸ç¨€ç–åŒ– | å¹³æ»‘è¿‡æ¸¡åˆ°Sparsemax |
| **Fine-tune** | 3000+ | Sparsemaxè·¯ç”±, ä½å­¦ä¹ ç‡ | ç²¾ç»†è°ƒæ•´ä¸“å®¶åˆ†å·¥ |

```python
def get_training_config(episode):
    """æ ¹æ®è®­ç»ƒé˜¶æ®µè¿”å›é…ç½®"""
    if episode < 1000:
        # Warmupé˜¶æ®µ
        return {
            'use_sparsemax': False,  # ä½¿ç”¨Softmax
            'learning_rate': 1e-4,
            'entropy_coef': 0.05,
            'load_balance_coef': 0.02,  # å¼ºåˆ¶å‡è¡¡
        }
    elif episode < 3000:
        # Transitioné˜¶æ®µ
        temp = 1.0 - 0.5 * (episode - 1000) / 2000  # 1.0 â†’ 0.5
        return {
            'use_sparsemax': True,
            'sparsemax_temp': temp,
            'learning_rate': 5e-5,
            'entropy_coef': 0.02,
            'load_balance_coef': 0.01,
        }
    else:
        # Fine-tuneé˜¶æ®µ
        return {
            'use_sparsemax': True,
            'sparsemax_temp': 0.5,
            'learning_rate': 1e-5,
            'entropy_coef': 0.01,
            'load_balance_coef': 0.005,
        }
```


### 7.2 Checkpointå’Œæ¢å¤

**ç­–ç•¥**:
1. æ¯100 episodesä¿å­˜checkpoint
2. ä¿å­˜æœ€ä½³æ¨¡å‹ (best_reward, best_score)
3. æ£€æµ‹åˆ°NaNæ—¶è‡ªåŠ¨å›æ»š

```python
def save_checkpoint(episode, policy_net, optimizer, stats, path):
    torch.save({
        'episode': episode,
        'policy_net': policy_net.state_dict(),
        'optimizer': optimizer.state_dict(),
        'stats': stats,
        'timestamp': time.time(),
    }, path)

def load_checkpoint(path, policy_net, optimizer):
    checkpoint = torch.load(path)
    policy_net.load_state_dict(checkpoint['policy_net'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    return checkpoint['episode'], checkpoint['stats']
```

---

## å…«ã€ç›‘æ§å’Œè¯Šæ–­æŒ‡æ ‡

### 8.1 å¿…é¡»ç›‘æ§çš„æŒ‡æ ‡

| æŒ‡æ ‡ | æ­£å¸¸èŒƒå›´ | å¼‚å¸¸ä¿¡å· | å¤„ç†æ–¹æ³• |
|------|----------|----------|----------|
| **episode_score** | é€æ¸ä¸Šå‡ | é•¿æœŸä¸å˜æˆ–ä¸‹é™ | æ£€æŸ¥å¥–åŠ±å¡‘å½¢ |
| **alpha_entropy** | 0.5-1.0 | <0.3 (å¡Œç¼©) æˆ– >1.2 (æ··ä¹±) | è°ƒæ•´load_balance_coef |
| **expert_usage** | æ¯ä¸ªä¸“å®¶10-40% | æŸä¸ªä¸“å®¶>80% | å¢åŠ load_balance_coef |
| **gat_attention_variance** | >0.1 | <0.05 (è¿‡å¹³æ»‘) | å‡å°‘GATå±‚æ•°æˆ–å¢åŠ dropout |
| **operator_activation_rate** | 10-30% | <5% æˆ– >50% | æ£€æŸ¥atomsæå–é€»è¾‘ |
| **gradient_norm** | <5.0 | >10.0 | é™ä½å­¦ä¹ ç‡æˆ–å¢åŠ è£å‰ª |
| **actor_loss** | é€æ¸ä¸‹é™ | éœ‡è¡æˆ–çˆ†ç‚¸ | é™ä½clip_ratio |
| **critic_loss** | é€æ¸ä¸‹é™ | ä¸æ”¶æ•› | å¢åŠ batch_size |


### 8.2 å®æ—¶ç›‘æ§ä»£ç 

```python
class TrainingMonitor:
    def __init__(self, log_interval=50):
        self.log_interval = log_interval
        self.metrics = defaultdict(list)
    
    def log(self, episode, metrics):
        """è®°å½•æŒ‡æ ‡"""
        for k, v in metrics.items():
            self.metrics[k].append(v)
        
        if episode % self.log_interval == 0:
            self.print_summary(episode)
            self.check_anomalies(episode)
    
    def print_summary(self, episode):
        """æ‰“å°æ‘˜è¦"""
        print(f"\n=== Episode {episode} Summary ===")
        for k, v in self.metrics.items():
            recent = v[-self.log_interval:]
            print(f"  {k}: {np.mean(recent):.4f} Â± {np.std(recent):.4f}")
    
    def check_anomalies(self, episode):
        """æ£€æŸ¥å¼‚å¸¸"""
        # æ£€æŸ¥ä¸“å®¶å¡Œç¼©
        if 'alpha_entropy' in self.metrics:
            recent_entropy = np.mean(self.metrics['alpha_entropy'][-50:])
            if recent_entropy < 0.3:
                print(f"âš ï¸ ä¸“å®¶å¡Œç¼©è­¦å‘Š: Î±ç†µ={recent_entropy:.4f}")
        
        # æ£€æŸ¥æ¢¯åº¦çˆ†ç‚¸
        if 'gradient_norm' in self.metrics:
            recent_grad = np.mean(self.metrics['gradient_norm'][-10:])
            if recent_grad > 10.0:
                print(f"âš ï¸ æ¢¯åº¦çˆ†ç‚¸è­¦å‘Š: æ¢¯åº¦èŒƒæ•°={recent_grad:.4f}")
        
        # æ£€æŸ¥GATè¿‡å¹³æ»‘
        if 'gat_attention_variance' in self.metrics:
            recent_var = np.mean(self.metrics['gat_attention_variance'][-50:])
            if recent_var < 0.05:
                print(f"âš ï¸ GATè¿‡å¹³æ»‘è­¦å‘Š: æ³¨æ„åŠ›æ–¹å·®={recent_var:.4f}")
```


### 8.3 å¯è§†åŒ–ç›‘æ§

**æ¨èå·¥å…·**: TensorBoard æˆ– Weights & Biases

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter(log_dir=f"runs/{exp_name}")

# è®°å½•æ ‡é‡
writer.add_scalar('Train/episode_score', score, episode)
writer.add_scalar('Train/alpha_entropy', alpha_entropy, episode)
writer.add_scalar('Train/gradient_norm', grad_norm, episode)

# è®°å½•ç›´æ–¹å›¾
writer.add_histogram('Train/alpha_distribution', alpha, episode)
writer.add_histogram('Train/operator_scores', operator_scores, episode)

# è®°å½•å›¾åƒ (GATæ³¨æ„åŠ›çƒ­å›¾)
writer.add_image('GAT/attention_heatmap', attention_heatmap, episode)
```

---

## ä¹ã€å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 9.1 ä¸“å®¶å¡Œç¼©

**ç—‡çŠ¶**:
- Î±ç†µ < 0.3
- æŸä¸ªä¸“å®¶æƒé‡ > 0.8
- å…¶ä»–ä¸“å®¶æ¢¯åº¦æ¥è¿‘0

**åŸå› **:
- Sparsemaxè¿‡æ—©æ”¶æ•›
- è´Ÿè½½å‡è¡¡æŸå¤±ä¸è¶³
- ä¸“å®¶åˆå§‹åŒ–ä¸å½“

**è§£å†³æ–¹æ¡ˆ**:
1. å¢åŠ Warmupé˜¶æ®µé•¿åº¦ (1000 â†’ 2000 episodes)
2. å¢åŠ load_balance_coef (0.01 â†’ 0.05)
3. æ·»åŠ ä¸“å®¶å¤šæ ·æ€§æŸå¤±
4. ä½¿ç”¨æ›´å¤§çš„Sparsemaxæ¸©åº¦


### 9.2 GATè¿‡å¹³æ»‘

**ç—‡çŠ¶**:
- GATæ³¨æ„åŠ›æ–¹å·® < 0.05
- æ‰€æœ‰èŠ‚ç‚¹åµŒå…¥è¶‹åŒ
- Intent Vectorå˜åŒ–å¾ˆå°

**åŸå› **:
- GATå±‚æ•°è¿‡å¤š
- å­¦ä¹ ç‡è¿‡å¤§
- ç¼ºä¹æ­£åˆ™åŒ–

**è§£å†³æ–¹æ¡ˆ**:
1. å‡å°‘GATå±‚æ•° (2 â†’ 1)
2. å¢åŠ Dropout (0.1 â†’ 0.2)
3. æ·»åŠ è¾¹Dropout
4. ä½¿ç”¨æ®‹å·®è¿æ¥ (å·²å®ç°)

### 9.3 æ¢¯åº¦çˆ†ç‚¸

**ç—‡çŠ¶**:
- æ¢¯åº¦èŒƒæ•° > 10.0
- æŸå¤±çªç„¶å˜ä¸ºNaN
- å‚æ•°æ›´æ–°è¿‡å¤§

**åŸå› **:
- å­¦ä¹ ç‡è¿‡å¤§
- æ¢¯åº¦è£å‰ªä¸è¶³
- ç½‘ç»œåˆå§‹åŒ–ä¸å½“

**è§£å†³æ–¹æ¡ˆ**:
1. é™ä½å­¦ä¹ ç‡ (1e-4 â†’ 5e-5)
2. å¢åŠ æ¢¯åº¦è£å‰ª (0.5 â†’ 1.0)
3. ä½¿ç”¨Xavieråˆå§‹åŒ–
4. æ·»åŠ LayerNorm (å·²å®ç°)

### 9.4 å¥–åŠ±ä¸æ”¶æ•›

**ç—‡çŠ¶**:
- episode_scoreé•¿æœŸä¸å˜
- ç­–ç•¥éœ‡è¡
- Î±æƒé‡æ··ä¹±

**åŸå› **:
- å¥–åŠ±å¡‘å½¢ä¸å½“
- æ¢ç´¢ä¸è¶³
- ä»·å€¼ä¼°è®¡åå·®

**è§£å†³æ–¹æ¡ˆ**:
1. è°ƒæ•´å¥–åŠ±æƒé‡
2. å¢åŠ ç†µæ­£åˆ™åŒ–
3. ä½¿ç”¨åŒCritic
4. å¢åŠ batch_size


---

## åã€é™çº§æ–¹æ¡ˆ

å¦‚æœV3è®­ç»ƒå¤±è´¥ï¼ŒæŒ‰ä»¥ä¸‹é¡ºåºå°è¯•é™çº§æ–¹æ¡ˆï¼š

### æ–¹æ¡ˆ1: å›ºå®šGAT (ä¸è®­ç»ƒ)

```python
# å†»ç»“GATå‚æ•°
for param in policy_net.gat.parameters():
    param.requires_grad = False

# åªè®­ç»ƒè·¯ç”±å™¨å’Œä¸“å®¶
optimizer = optim.Adam([
    {'params': policy_net.router.parameters()},
    {'params': policy_net.experts.parameters()},
    {'params': policy_net.critic.parameters()},
], lr=1e-4)
```

### æ–¹æ¡ˆ2: ä½¿ç”¨Softmaxè·¯ç”±

```python
# ç¦ç”¨Sparsemaxï¼Œä½¿ç”¨Softmax
policy_net = GATGuidedMoEPolicy(
    use_sparsemax=False  # ä½¿ç”¨Softmax
)
```

### æ–¹æ¡ˆ3: å‡å°‘ä¸“å®¶æ•°é‡

```python
# ä»4ä¸ªä¸“å®¶å‡å°‘åˆ°2ä¸ª
policy_net = GATGuidedMoEPolicy(
    num_experts=2  # Survival + General
)
```

### æ–¹æ¡ˆ4: å›é€€åˆ°V2 + GATç‰¹å¾

```python
# ä½¿ç”¨GATæå–ç‰¹å¾ï¼Œä½†ç”¨V2çš„è·¯ç”±æ–¹å¼
h_logic, _, _ = gat(atoms=atoms)
state_with_gat = np.concatenate([state, h_logic.cpu().numpy()])

# ä½¿ç”¨V2ç½‘ç»œ
policy_net_v2 = MultiChannelPolicyNet(
    state_dim=115 + 256,  # åŸå§‹state + GATç‰¹å¾
    use_gumbel=True
)
```


---

## åä¸€ã€å®Œæ•´è®­ç»ƒè„šæœ¬æ¨¡æ¿

```python
def train_v3(args):
    """V3è®­ç»ƒä¸»å¾ªç¯ - åŒ…å«æ‰€æœ‰ç¨³å®šæ€§æªæ–½"""
    
    # 1. åˆå§‹åŒ–
    device = get_device()
    policy_net = GATGuidedMoEPolicy(...).to(device)
    optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)
    lr_scheduler = get_lr_scheduler(optimizer)
    
    # 2. ç›‘æ§å™¨
    monitor = TrainingMonitor(log_interval=50)
    nan_detector = NaNDetector(policy_net)
    reward_normalizer = RewardNormalizer()
    
    # 3. è®­ç»ƒå¾ªç¯
    for episode in range(args.episodes):
        # 3.1 è·å–å½“å‰é˜¶æ®µé…ç½®
        config = get_training_config(episode)
        policy_net.use_sparsemax = config['use_sparsemax']
        
        # 3.2 Episodeå¾ªç¯
        obs, info = env.reset()
        done = False
        episode_metrics = defaultdict(list)
        
        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            state, atoms = extract_state_and_atoms(obs)
            logits, alpha, value, aux_info = policy_net(state, atoms)
            
            # è®°å½•æŒ‡æ ‡
            episode_metrics['alpha_entropy'].append(
                -(alpha * torch.log(alpha + 1e-8)).sum().item()
            )
            episode_metrics['operator_activation_rate'].append(
                (aux_info['operator_scores'] > 0.5).float().mean().item()
            )
            
            # æ‰§è¡ŒåŠ¨ä½œ
            action = Categorical(logits=logits).sample()
            obs, reward, done, truncated, info = env.step(action.item())
            
            # å¥–åŠ±å¡‘å½¢
            shaped_reward = compute_v3_reward(reward, aux_info, {'alpha': alpha})
            normalized_reward = reward_normalizer.normalize(shaped_reward)
            
            # å­˜å‚¨ç»éªŒ
            trainer.buffer.add(state, action, normalized_reward, ...)
        
        # 3.3 æ›´æ–°ç½‘ç»œ
        if len(trainer.buffer) >= trainer.batch_size:
            # ä¿å­˜checkpoint (ç”¨äºNaNå›æ»š)
            nan_detector.save_checkpoint()
            
            # è®¡ç®—æŸå¤±
            actor_loss, critic_loss = trainer.compute_losses()
            
            # è¾…åŠ©æŸå¤±
            lb_loss = load_balance_loss(alpha_history)
            div_loss = expert_diversity_loss(expert_logits_history)
            
            total_loss = (
                actor_loss + 
                0.5 * critic_loss + 
                config['load_balance_coef'] * lb_loss +
                0.01 * div_loss
            )
            
            # æ£€æŸ¥NaN
            if nan_detector.check_and_rollback(total_loss):
                continue
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            total_loss.backward()
            
            # è®°å½•æ¢¯åº¦èŒƒæ•°
            grad_norm = log_gradient_norms(policy_net)
            episode_metrics['gradient_norm'].append(grad_norm)
            
            # æ¢¯åº¦è£å‰ª
            nn.utils.clip_grad_norm_(policy_net.parameters(), 1.0)
            
            # æ›´æ–°
            optimizer.step()
            lr_scheduler.step()
        
        # 3.4 ç›‘æ§
        monitor.log(episode, {
            'episode_score': info['score'],
            'alpha_entropy': np.mean(episode_metrics['alpha_entropy']),
            'operator_activation_rate': np.mean(episode_metrics['operator_activation_rate']),
            'gradient_norm': np.mean(episode_metrics['gradient_norm']),
        })
        
        # 3.5 ä¿å­˜checkpoint
        if episode % 100 == 0:
            save_checkpoint(episode, policy_net, optimizer, ...)
```


---

## åäºŒã€å®æ–½æ£€æŸ¥æ¸…å•

åœ¨å¼€å§‹è®­ç»ƒå‰ï¼Œç¡®ä¿ä»¥ä¸‹æ‰€æœ‰é¡¹ç›®éƒ½å·²å®Œæˆï¼š

### ç½‘ç»œæ¶æ„ âœ…/âŒ

- [ ] GATä½¿ç”¨2å±‚ï¼Œå¸¦æ®‹å·®è¿æ¥
- [ ] æ¯å±‚åæœ‰LayerNorm
- [ ] è·¯ç”±å™¨ä½¿ç”¨Sparsemax (å¸¦æ¸©åº¦é€€ç«)
- [ ] ä¸“å®¶è¾“å‡ºå±‚å°å¢ç›Šåˆå§‹åŒ– (0.01)
- [ ] Criticæœ‰ä»·å€¼è£å‰ª

### è®­ç»ƒè¶…å‚æ•° âœ…/âŒ

- [ ] å­¦ä¹ ç‡è®¾ä¸º1e-4 (æ¯”V2æ›´å°)
- [ ] ä½¿ç”¨å­¦ä¹ ç‡Warmup (1000 steps)
- [ ] PPO clip_ratioè®¾ä¸º0.15
- [ ] batch_size >= 256
- [ ] æ¢¯åº¦è£å‰ªmax_norm=1.0

### è¾…åŠ©æŸå¤± âœ…/âŒ

- [ ] å®ç°load_balance_loss
- [ ] å®ç°expert_diversity_loss
- [ ] è®¾ç½®åˆé€‚çš„æŸå¤±æƒé‡
- [ ] (å¯é€‰) å®ç°next_intent_prediction_loss

### æ•°å€¼ç¨³å®šæ€§ âœ…/âŒ

- [ ] æ‰€æœ‰logitsåšnan_to_numå’Œclamp
- [ ] å®ç°NaNæ£€æµ‹å’Œå›æ»š
- [ ] å¥–åŠ±å½’ä¸€åŒ–
- [ ] æƒé‡åˆå§‹åŒ–æ£€æŸ¥

### ç›‘æ§å’Œè¯Šæ–­ âœ…/âŒ

- [ ] å®ç°TrainingMonitorç±»
- [ ] è®°å½•æ‰€æœ‰å…³é”®æŒ‡æ ‡
- [ ] è®¾ç½®å¼‚å¸¸æ£€æµ‹é˜ˆå€¼
- [ ] (å¯é€‰) é›†æˆTensorBoard

### Warmupæœºåˆ¶ âœ…/âŒ

- [ ] å‰1000 episodesä½¿ç”¨Softmax
- [ ] 1000-3000 episodesæ¸©åº¦é€€ç«
- [ ] 3000+ episodesä½¿ç”¨Sparsemax (temp=0.5)
- [ ] å­¦ä¹ ç‡éšé˜¶æ®µè°ƒæ•´

### Checkpointå’Œæ¢å¤ âœ…/âŒ

- [ ] æ¯100 episodesä¿å­˜checkpoint
- [ ] ä¿å­˜æœ€ä½³æ¨¡å‹
- [ ] å®ç°checkpointåŠ è½½é€»è¾‘
- [ ] NaNæ—¶è‡ªåŠ¨å›æ»š


---

## åä¸‰ã€é¢„æœŸè®­ç»ƒæ›²çº¿

### æ­£å¸¸è®­ç»ƒæ›²çº¿

```
Episode Score:
  0-1000:    50-100   (Warmup, æ¢ç´¢)
  1000-3000: 100-300  (Transition, ä¸“å®¶åˆ†å·¥å½¢æˆ)
  3000-5000: 300-600  (Fine-tune, ç¨³å®šæå‡)
  5000+:     600-800+ (æ”¶æ•›)

Alpha Entropy:
  0-1000:    1.2-1.4  (Softmax, é«˜ç†µ)
  1000-3000: 1.0-0.6  (é€€ç«, é€æ¸ç¨€ç–)
  3000+:     0.5-0.8  (Sparsemax, ç¨³å®š)

Expert Usage (ç†æƒ³):
  Survival:    20-30%
  Combat:      25-35%
  Exploration: 25-35%
  General:     10-20%

Gradient Norm:
  å…¨ç¨‹:       1.0-3.0  (ç¨³å®š)
  å¼‚å¸¸:       >10.0    (éœ€è¦å¹²é¢„)
```

### å¼‚å¸¸è®­ç»ƒæ›²çº¿

```
ä¸“å®¶å¡Œç¼©:
  Alpha Entropy < 0.3
  æŸä¸ªä¸“å®¶ > 80%
  â†’ å¢åŠ load_balance_coef

GATè¿‡å¹³æ»‘:
  Attention Variance < 0.05
  Operator Activation Rate < 5%
  â†’ å‡å°‘GATå±‚æ•°æˆ–å¢åŠ Dropout

æ¢¯åº¦çˆ†ç‚¸:
  Gradient Norm > 10.0
  Lossçªç„¶å˜NaN
  â†’ é™ä½å­¦ä¹ ç‡ï¼Œå¢åŠ è£å‰ª

å¥–åŠ±ä¸æ”¶æ•›:
  Scoreé•¿æœŸä¸å˜
  Alphaæƒé‡æ··ä¹±
  â†’ è°ƒæ•´å¥–åŠ±å¡‘å½¢ï¼Œå¢åŠ æ¢ç´¢
```


---

## åå››ã€å‚è€ƒæ–‡çŒ®

### ç›¸å…³æŠ€æœ¯

1. **GATç¨³å®šæ€§**:
   - "Graph Attention Networks" (VeliÄkoviÄ‡ et al., 2018)
   - "How to Train Your Graph Neural Network" (Dwivedi et al., 2020)

2. **MoEè®­ç»ƒ**:
   - "Switch Transformers" (Fedus et al., 2021) - è´Ÿè½½å‡è¡¡
   - "GShard" (Lepikhin et al., 2020) - ä¸“å®¶å¹¶è¡Œ

3. **Sparsemax**:
   - "From Softmax to Sparsemax" (Martins & Astudillo, 2016)

4. **PPOç¨³å®šæ€§**:
   - "Implementation Matters in Deep RL" (Engstrom et al., 2020)
   - "What Matters in On-Policy RL" (Andrychowicz et al., 2021)

### V1/V2ç»éªŒ

- V1: Softmaxè·¯ç”±ç¨³å®šä½†åˆ†å·¥ä¸æ˜ç¡®
- V2: Gumbelè·¯ç”±å®¹æ˜“å¡Œç¼©ï¼Œéœ€è¦å¼ºæ­£åˆ™åŒ–
- V3: Sparsemaxæ˜¯æŠ˜ä¸­æ–¹æ¡ˆï¼Œéœ€è¦Warmup

---

## åäº”ã€æ€»ç»“

### æ ¸å¿ƒåŸåˆ™

1. **æ¸è¿›å¼è®­ç»ƒ**: Warmup â†’ Transition â†’ Fine-tune
2. **å¤šé‡æ­£åˆ™åŒ–**: è´Ÿè½½å‡è¡¡ + å¤šæ ·æ€§ + ç†µæ­£åˆ™
3. **ä¸¥æ ¼ç›‘æ§**: å®æ—¶æ£€æµ‹å¼‚å¸¸ï¼ŒåŠæ—¶å¹²é¢„
4. **æ•°å€¼ç¨³å®š**: NaNæ£€æµ‹ã€æ¢¯åº¦è£å‰ªã€å¥–åŠ±å½’ä¸€åŒ–
5. **é™çº§å‡†å¤‡**: å¤šä¸ªå¤‡é€‰æ–¹æ¡ˆï¼Œé¿å…å…¨ç›˜å¤±è´¥

### æˆåŠŸæ ‡å‡†

- **çŸ­æœŸ** (1å‘¨): è®­ç»ƒç¨³å®šï¼Œæ— NaNï¼Œä¸“å®¶æ— å¡Œç¼©
- **ä¸­æœŸ** (2å‘¨): Score > 600, Î±ç†µ < 1.0
- **é•¿æœŸ** (1æœˆ): Score > 800, æ ·æœ¬æ•ˆç‡ > 1.5x V1

### ä¸‹ä¸€æ­¥

1. å®ç°è®­ç»ƒè„šæœ¬ `ablation_v3/train/train_v3_gat_moe.py`
2. é›†æˆæ‰€æœ‰ç¨³å®šæ€§æªæ–½
3. å°è§„æ¨¡æµ‹è¯• (100 episodes)
4. å…¨é¢è®­ç»ƒ (10000 episodes)

---

**æ–‡æ¡£çŠ¶æ€**: âœ… å®Œæˆ  
**å‡†å¤‡åº¦**: âœ… å¯ä»¥å¼€å§‹å®ç°è®­ç»ƒè„šæœ¬  
**ä¿¡å¿ƒ**: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ (5/5)

