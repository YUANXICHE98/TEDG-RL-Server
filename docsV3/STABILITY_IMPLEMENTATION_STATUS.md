# V3 ç¨³å®šæ€§æªæ–½å®ç°çŠ¶æ€

> **æ›´æ–°æ—¶é—´**: 2025-01-05  
> **ç›®çš„**: å¯¹æ¯”checklistä¸­çš„æªæ–½ä¸å½“å‰ä»£ç å®ç°

---

## âœ… å·²å®ç°çš„ç¨³å®šæ€§æªæ–½

### 1. ç½‘ç»œæ¶æ„å±‚

#### GATå±‚ (src/core/hypergraph_gat.py)
- âœ… **2å±‚GAT**: é™åˆ¶å±‚æ•°ï¼Œé¿å…è¿‡å¹³æ»‘
- âœ… **æ®‹å·®è¿æ¥**: `x2 = x1 + GAT(x1)` ä¿æŒæ¢¯åº¦æµ
- âœ… **LayerNorm**: æ¯å±‚åå½’ä¸€åŒ–
- âœ… **å¤šå¤´æ³¨æ„åŠ›**: 4ä¸ªå¤´
- âœ… **Dropout**: 0.1

#### è·¯ç”±å™¨ (src/core/networks_v3_gat_moe.py - CausalRouter)
- âœ… **Sparsemaxæ¿€æ´»**: è‡ªåŠ¨ç¨€ç–åŒ–
- âœ… **3å±‚MLP**: 512â†’128â†’64â†’4
- âœ… **LayerNorm**: æ¯å±‚åå½’ä¸€åŒ–
- âœ… **Warmupæ”¯æŒ**: `use_sparsemax`å‚æ•°å¯åˆ‡æ¢Softmax/Sparsemax
- âœ… **æ•°å€¼ç¨³å®š**: logitsåšnan_to_numå’Œclamp

#### ä¸“å®¶ç½‘ç»œ (src/core/networks_v3_gat_moe.py - SemanticExpert)
- âœ… **ç‹¬ç«‹MLP**: æ¯ä¸ªä¸“å®¶2å±‚MLP
- âœ… **LayerNorm**: ç¨³å®šæ¿€æ´»å€¼
- âœ… **å°å¢ç›Šåˆå§‹åŒ–**: è¾“å‡ºå±‚gain=0.01

#### Criticç½‘ç»œ (src/core/networks_v3_gat_moe.py)
- âœ… **åŒæµè¾“å…¥**: h_vis + h_logic (512ç»´)
- âœ… **3å±‚MLP**: 512â†’256â†’128â†’1
- âœ… **LayerNorm**: ç¨³å®šè®­ç»ƒ

### 2. æ•°å€¼ç¨³å®šæ€§

#### NaN/Infå¤„ç†
- âœ… **è·¯ç”±å™¨logits**: `torch.nan_to_num(...).clamp(-20.0, 20.0)`
- âœ… **èåˆlogits**: `torch.nan_to_num(...).clamp(-20.0, 20.0)`
- âœ… **ä»·å€¼ä¼°è®¡**: `torch.nan_to_num(...)`

#### æ¢¯åº¦è£å‰ª (src/core/ppo_trainer.py)
- âœ… **å·²å®ç°**: `nn.utils.clip_grad_norm_(parameters, 0.5)`
- âš ï¸ **éœ€è°ƒæ•´**: V3æ¨èmax_norm=1.0

### 3. è¾…åŠ©åŠŸèƒ½

#### ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
- âœ… **get_expert_usage_stats**: åˆ†æÎ±æƒé‡åˆ†å¸ƒ
- âœ… **dominant_counts**: ç»Ÿè®¡ä¸»å¯¼ä¸“å®¶

#### åŠ¨ä½œåˆ†å¸ƒ
- âœ… **get_action_distribution**: ç”¨äºPPOé‡‡æ ·

---

## âŒ å¾…å®ç°çš„ç¨³å®šæ€§æªæ–½

### 1. è¾…åŠ©æŸå¤±å‡½æ•° (éœ€è¦åœ¨è®­ç»ƒè„šæœ¬ä¸­å®ç°)

#### è´Ÿè½½å‡è¡¡æŸå¤± (é˜²æ­¢ä¸“å®¶å¡Œç¼©)
```python
def load_balance_loss(alpha, num_experts=4):
    """é¼“åŠ±æ¯ä¸ªä¸“å®¶è¢«å‡åŒ€ä½¿ç”¨"""
    expert_usage = alpha.mean(dim=0)
    target_usage = torch.ones_like(expert_usage) / num_experts
    return F.mse_loss(expert_usage, target_usage)
```
**çŠ¶æ€**: âŒ æœªå®ç°  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ğŸ”¥ æé«˜ (é˜²æ­¢å¡Œç¼©çš„å…³é”®)

#### ä¸“å®¶å¤šæ ·æ€§æŸå¤± (é¼“åŠ±å·®å¼‚åŒ–)
```python
def expert_diversity_loss(expert_logits):
    """æœ€å°åŒ–ä¸“å®¶é—´ä½™å¼¦ç›¸ä¼¼åº¦"""
    num_experts = expert_logits.size(1)
    diversity = 0.0
    for i in range(num_experts):
        for j in range(i+1, num_experts):
            cos_sim = F.cosine_similarity(
                expert_logits[:, i, :], 
                expert_logits[:, j, :], 
                dim=-1
            ).mean()
            diversity += cos_sim
    return diversity / (num_experts * (num_experts - 1) / 2)
```
**çŠ¶æ€**: âŒ æœªå®ç°  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ é«˜

#### GATæ³¨æ„åŠ›æ­£åˆ™åŒ–
```python
def attention_regularization(attention_weights, target_sparsity=0.3):
    """é¼“åŠ±é€‚åº¦ç¨€ç–çš„æ³¨æ„åŠ›"""
    # è®¡ç®—Giniç³»æ•°
    sorted_weights, _ = torch.sort(attention_weights)
    n = len(sorted_weights)
    index = torch.arange(1, n+1, device=sorted_weights.device)
    gini = (2 * (index * sorted_weights).sum()) / (n * sorted_weights.sum()) - (n+1)/n
    return (gini - target_sparsity) ** 2
```
**çŠ¶æ€**: âŒ æœªå®ç°  
**ä¼˜å…ˆçº§**: ğŸ”¥ ä¸­


### 2. è®­ç»ƒæµç¨‹æœºåˆ¶ (éœ€è¦åœ¨è®­ç»ƒè„šæœ¬ä¸­å®ç°)

#### ä¸‰é˜¶æ®µè®­ç»ƒé…ç½®
```python
def get_training_config(episode):
    """æ ¹æ®è®­ç»ƒé˜¶æ®µè¿”å›é…ç½®"""
    if episode < 1000:
        # Warmupé˜¶æ®µ
        return {
            'use_sparsemax': False,
            'learning_rate': 1e-4,
            'load_balance_coef': 0.02,
        }
    elif episode < 3000:
        # Transitioné˜¶æ®µ
        temp = 1.0 - 0.5 * (episode - 1000) / 2000
        return {
            'use_sparsemax': True,
            'sparsemax_temp': temp,
            'learning_rate': 5e-5,
            'load_balance_coef': 0.01,
        }
    else:
        # Fine-tuneé˜¶æ®µ
        return {
            'use_sparsemax': True,
            'sparsemax_temp': 0.5,
            'learning_rate': 1e-5,
            'load_balance_coef': 0.005,
        }
```
**çŠ¶æ€**: âŒ æœªå®ç°  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ğŸ”¥ æé«˜ (æ ¸å¿ƒè®­ç»ƒç­–ç•¥)

#### å­¦ä¹ ç‡Warmupå’Œé€€ç«
```python
def get_lr_scheduler(optimizer, warmup_steps=1000, max_steps=100000):
    def lr_lambda(step):
        if step < warmup_steps:
            return step / warmup_steps
        else:
            return 1.0
    
    warmup_scheduler = LambdaLR(optimizer, lr_lambda)
    cosine_scheduler = CosineAnnealingLR(optimizer, T_max=max_steps-warmup_steps)
    
    return warmup_scheduler, cosine_scheduler
```
**çŠ¶æ€**: âŒ æœªå®ç°  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ é«˜

#### NaNæ£€æµ‹å’Œè‡ªåŠ¨å›æ»š
```python
class NaNDetector:
    def __init__(self, model):
        self.model = model
        self.last_good_state = None
    
    def save_checkpoint(self):
        self.last_good_state = {
            k: v.clone() for k, v in self.model.state_dict().items()
        }
    
    def check_and_rollback(self, loss):
        if torch.isnan(loss) or torch.isinf(loss):
            print("âš ï¸ NaN/Inf detected! Rolling back...")
            if self.last_good_state:
                self.model.load_state_dict(self.last_good_state)
            return True
        return False
```
**çŠ¶æ€**: âŒ æœªå®ç°  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ é«˜

### 3. ç›‘æ§å’Œè¯Šæ–­ (éœ€è¦åœ¨è®­ç»ƒè„šæœ¬ä¸­å®ç°)

#### TrainingMonitorç±»
```python
class TrainingMonitor:
    def __init__(self, log_interval=50):
        self.log_interval = log_interval
        self.metrics = defaultdict(list)
    
    def log(self, episode, metrics):
        for k, v in metrics.items():
            self.metrics[k].append(v)
        
        if episode % self.log_interval == 0:
            self.print_summary(episode)
            self.check_anomalies(episode)
    
    def check_anomalies(self, episode):
        # æ£€æŸ¥ä¸“å®¶å¡Œç¼©
        if 'alpha_entropy' in self.metrics:
            recent_entropy = np.mean(self.metrics['alpha_entropy'][-50:])
            if recent_entropy < 0.3:
                print(f"âš ï¸ ä¸“å®¶å¡Œç¼©è­¦å‘Š: Î±ç†µ={recent_entropy:.4f}")
        
        # æ£€æŸ¥æ¢¯åº¦çˆ†ç‚¸
        if 'gradient_norm' in self.metrics:
            recent_grad = np.mean(self.metrics['gradient_norm'][-10:])
            if recent_grad > 10.0:
                print(f"âš ï¸ æ¢¯åº¦çˆ†ç‚¸è­¦å‘Š: æ¢¯åº¦èŒƒæ•°={recent_grad:.4f}")
```
**çŠ¶æ€**: âŒ æœªå®ç°  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ é«˜

#### æ¢¯åº¦èŒƒæ•°ç›‘æ§
```python
def log_gradient_norms(model):
    total_norm = 0.0
    for p in model.parameters():
        if p.grad is not None:
            param_norm = p.grad.data.norm(2)
            total_norm += param_norm.item() ** 2
    total_norm = total_norm ** 0.5
    
    if total_norm > 10.0:
        print(f"âš ï¸ æ¢¯åº¦çˆ†ç‚¸è­¦å‘Š: {total_norm:.4f}")
    
    return total_norm
```
**çŠ¶æ€**: âŒ æœªå®ç°  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ é«˜

### 4. å¥–åŠ±å¤„ç† (éœ€è¦åœ¨è®­ç»ƒè„šæœ¬ä¸­å®ç°)

#### å¥–åŠ±å½’ä¸€åŒ–
```python
class RewardNormalizer:
    def __init__(self, clip_range=10.0):
        self.mean = 0.0
        self.std = 1.0
        self.clip_range = clip_range
        self.count = 0
    
    def update(self, reward):
        self.count += 1
        delta = reward - self.mean
        self.mean += delta / self.count
        self.std = np.sqrt((self.std**2 * (self.count-1) + delta**2) / self.count)
    
    def normalize(self, reward):
        normalized = (reward - self.mean) / (self.std + 1e-8)
        return np.clip(normalized, -self.clip_range, self.clip_range)
```
**çŠ¶æ€**: âŒ æœªå®ç°  
**ä¼˜å…ˆçº§**: ğŸ”¥ ä¸­

#### V3å¥–åŠ±å¡‘å½¢
```python
def compute_v3_reward(env_reward, gat_info, expert_info):
    """V3å¢å¼ºçš„å¥–åŠ±å¡‘å½¢"""
    r_base = env_reward / 1000.0
    
    # GATå¥–åŠ±: é¼“åŠ±æ¿€æ´»æœ‰æ„ä¹‰çš„Operator
    r_gat = 0.01 * gat_info['operator_activation_rate']
    
    # ä¸“å®¶å¥–åŠ±: é¼“åŠ±æ˜ç¡®çš„ä¸“å®¶é€‰æ‹©
    alpha_entropy = -(expert_info['alpha'] * np.log(expert_info['alpha'] + 1e-8)).sum()
    r_expert = -0.01 * alpha_entropy
    
    return r_base + r_gat + r_expert
```
**çŠ¶æ€**: âŒ æœªå®ç°  
**ä¼˜å…ˆçº§**: ğŸ”¥ ä¸­

---

## ğŸ”§ éœ€è¦è°ƒæ•´çš„ç°æœ‰å®ç°

### 1. PPO Trainer (src/core/ppo_trainer.py)

#### æ¢¯åº¦è£å‰ª
```python
# å½“å‰: max_norm=0.5
nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)

# V3æ¨è: max_norm=1.0
nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)
```
**çŠ¶æ€**: âš ï¸ éœ€è°ƒæ•´  
**ä¼˜å…ˆçº§**: ğŸ”¥ ä¸­

#### å­¦ä¹ ç‡
```python
# å½“å‰: 3e-4
self.optimizer = optim.Adam(self.policy_net.parameters(), lr=3e-4)

# V3æ¨è: 1e-4
self.optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-4)
```
**çŠ¶æ€**: âš ï¸ éœ€è°ƒæ•´  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ é«˜

#### PPOè¶…å‚æ•°
```python
# å½“å‰å€¼ â†’ V3æ¨èå€¼
clip_ratio: 0.2 â†’ 0.15
batch_size: 64 â†’ 256
ppo_epochs: 3 â†’ 4
gamma: 0.99 â†’ 0.995
gae_lambda: 0.95 â†’ 0.97
entropy_coef: 0.05 â†’ 0.01
alpha_entropy_coef: 0.1 â†’ 0.05
```
**çŠ¶æ€**: âš ï¸ éœ€è°ƒæ•´  
**ä¼˜å…ˆçº§**: ğŸ”¥ğŸ”¥ é«˜

### 2. Sparsemaxå®ç° (src/core/networks_v3_gat_moe.py)

#### å½“å‰å®ç°
```python
def sparsemax(logits, dim=-1):
    # ç®€åŒ–å®ç°: top-k + softmax
    k = max(2, logits.size(dim) // 2)
    topk_values, topk_indices = torch.topk(logits, k, dim=dim)
    topk_probs = F.softmax(topk_values, dim=dim)
    output = torch.zeros_like(logits)
    output.scatter_(dim, topk_indices, topk_probs)
    return output
```
**çŠ¶æ€**: âš ï¸ ç®€åŒ–ç‰ˆï¼Œå¯ç”¨ä½†ä¸å®Œç¾  
**ä¼˜å…ˆçº§**: ğŸ”¥ ä½ (çŸ­æœŸå¯æ¥å—)

#### å®Œæ•´å®ç° (å¯é€‰)
```python
def sparsemax(logits, dim=-1):
    """å®Œæ•´çš„Sparsemaxå®ç°"""
    # æ’åº
    sorted_logits, _ = torch.sort(logits, dim=dim, descending=True)
    
    # è®¡ç®—é˜ˆå€¼
    cumsum = torch.cumsum(sorted_logits, dim=dim)
    k = torch.arange(1, logits.size(dim) + 1, device=logits.device)
    support = (1 + k * sorted_logits) > cumsum
    k_z = support.sum(dim=dim, keepdim=True)
    tau = (cumsum.gather(dim, k_z - 1) - 1) / k_z
    
    # åº”ç”¨é˜ˆå€¼
    output = torch.clamp(logits - tau, min=0)
    return output
```
**çŠ¶æ€**: âŒ æœªå®ç° (å¯é€‰ä¼˜åŒ–)  
**ä¼˜å…ˆçº§**: ğŸ”¥ ä½

---

## ğŸ“Š å®ç°ä¼˜å…ˆçº§æ€»ç»“

### ğŸ”¥ğŸ”¥ğŸ”¥ æé«˜ä¼˜å…ˆçº§ (å¿…é¡»å®ç°)

1. **è´Ÿè½½å‡è¡¡æŸå¤±** - é˜²æ­¢ä¸“å®¶å¡Œç¼©çš„å…³é”®
2. **ä¸‰é˜¶æ®µè®­ç»ƒé…ç½®** - Warmup â†’ Transition â†’ Fine-tune
3. **è°ƒæ•´PPOè¶…å‚æ•°** - å­¦ä¹ ç‡ã€clip_ratioã€batch_sizeç­‰

### ğŸ”¥ğŸ”¥ é«˜ä¼˜å…ˆçº§ (å¼ºçƒˆæ¨è)

4. **ä¸“å®¶å¤šæ ·æ€§æŸå¤±** - é¼“åŠ±ä¸“å®¶å·®å¼‚åŒ–
5. **å­¦ä¹ ç‡Warmupå’Œé€€ç«** - ç¨³å®šè®­ç»ƒ
6. **NaNæ£€æµ‹å’Œå›æ»š** - é˜²æ­¢å´©æºƒ
7. **TrainingMonitorç±»** - å®æ—¶ç›‘æ§
8. **æ¢¯åº¦èŒƒæ•°ç›‘æ§** - æ£€æµ‹çˆ†ç‚¸

### ğŸ”¥ ä¸­ä¼˜å…ˆçº§ (å»ºè®®å®ç°)

9. **GATæ³¨æ„åŠ›æ­£åˆ™åŒ–** - é˜²æ­¢è¿‡å¹³æ»‘
10. **å¥–åŠ±å½’ä¸€åŒ–** - ç¨³å®šä»·å€¼ä¼°è®¡
11. **V3å¥–åŠ±å¡‘å½¢** - åˆ©ç”¨GATå’Œä¸“å®¶ä¿¡æ¯
12. **è°ƒæ•´æ¢¯åº¦è£å‰ª** - max_norm=1.0

### ä½ä¼˜å…ˆçº§ (å¯é€‰ä¼˜åŒ–)

13. **å®Œæ•´Sparsemaxå®ç°** - å½“å‰ç®€åŒ–ç‰ˆå¯ç”¨
14. **åŒCritic** - å‡å°‘è¿‡ä¼°è®¡
15. **è¾¹Dropout** - GATæ­£åˆ™åŒ–

---

## ğŸ“ å®ç°å»ºè®®

### ç«‹å³è¡ŒåŠ¨ (å¼€å§‹å®ç°è®­ç»ƒè„šæœ¬æ—¶)

1. åˆ›å»º `ablation_v3/train/train_v3_gat_moe.py`
2. å®ç°è´Ÿè½½å‡è¡¡æŸå¤±å’Œä¸“å®¶å¤šæ ·æ€§æŸå¤±
3. å®ç°ä¸‰é˜¶æ®µè®­ç»ƒé…ç½®
4. è°ƒæ•´PPOè¶…å‚æ•°
5. å®ç°TrainingMonitorç±»
6. å®ç°NaNæ£€æµ‹å’Œå›æ»š

### ä»£ç ç»“æ„å»ºè®®

```python
# ablation_v3/train/train_v3_gat_moe.py

# 1. è¾…åŠ©æŸå¤±å‡½æ•°
def load_balance_loss(alpha): ...
def expert_diversity_loss(expert_logits): ...

# 2. è®­ç»ƒé…ç½®
def get_training_config(episode): ...

# 3. ç›‘æ§ç±»
class TrainingMonitor: ...
class NaNDetector: ...
class RewardNormalizer: ...

# 4. ä¸»è®­ç»ƒå¾ªç¯
def train_v3(args):
    # åˆå§‹åŒ–
    policy_net = GATGuidedMoEPolicy(...)
    optimizer = optim.Adam(policy_net.parameters(), lr=1e-4)
    monitor = TrainingMonitor()
    nan_detector = NaNDetector(policy_net)
    
    # è®­ç»ƒå¾ªç¯
    for episode in range(args.episodes):
        config = get_training_config(episode)
        policy_net.use_sparsemax = config['use_sparsemax']
        
        # Episodeå¾ªç¯
        ...
        
        # æ›´æ–°ç½‘ç»œ
        actor_loss, critic_loss = ...
        lb_loss = load_balance_loss(alpha_history)
        div_loss = expert_diversity_loss(expert_logits_history)
        
        total_loss = (
            actor_loss + 
            0.5 * critic_loss + 
            config['load_balance_coef'] * lb_loss +
            0.01 * div_loss
        )
        
        # NaNæ£€æµ‹
        if nan_detector.check_and_rollback(total_loss):
            continue
        
        # åå‘ä¼ æ’­
        ...
        
        # ç›‘æ§
        monitor.log(episode, metrics)
```

---

## âœ… æ£€æŸ¥æ¸…å•

åœ¨å¼€å§‹è®­ç»ƒå‰ï¼Œç¡®ä¿ï¼š

- [ ] è´Ÿè½½å‡è¡¡æŸå¤±å·²å®ç°
- [ ] ä¸“å®¶å¤šæ ·æ€§æŸå¤±å·²å®ç°
- [ ] ä¸‰é˜¶æ®µè®­ç»ƒé…ç½®å·²å®ç°
- [ ] PPOè¶…å‚æ•°å·²è°ƒæ•´ (lr=1e-4, clip=0.15, batch=256)
- [ ] TrainingMonitorå·²å®ç°
- [ ] NaNæ£€æµ‹å’Œå›æ»šå·²å®ç°
- [ ] æ¢¯åº¦èŒƒæ•°ç›‘æ§å·²å®ç°
- [ ] å¥–åŠ±å½’ä¸€åŒ–å·²å®ç°
- [ ] å­¦ä¹ ç‡Warmupå·²å®ç°
- [ ] Checkpointä¿å­˜é€»è¾‘å·²å®ç°

---

**æ€»ç»“**: æ ¸å¿ƒç½‘ç»œæ¶æ„å·²å®ç°ï¼Œä½†è®­ç»ƒæµç¨‹ä¸­çš„ç¨³å®šæ€§æªæ–½ï¼ˆè¾…åŠ©æŸå¤±ã€ç›‘æ§ã€Warmupç­‰ï¼‰éœ€è¦åœ¨è®­ç»ƒè„šæœ¬ä¸­å®ç°ã€‚

