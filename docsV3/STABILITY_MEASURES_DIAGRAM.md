# V3 训练稳定性措施 - 架构图

```
┌─────────────────────────────────────────────────────────────────────┐
│                        V3 训练稳定性架构                              │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│  1. 网络架构层 (Architecture Level)                                  │
├─────────────────────────────────────────────────────────────────────┤
│                                                                       │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐          │
│  │  GAT Layer   │───▶│  Router      │───▶│  Experts     │          │
│  │              │    │              │    │              │          │
│  │ • 2层限制    │    │ • Sparsemax  │    │ • 4个专家    │          │
│  │ • 残差连接   │    │ • 温度退火   │    │ • 独立MLP    │          │
│  │ • LayerNorm  │    │ • LayerNorm  │    │ • 小增益初始化│          │
│  │ • Dropout    │    │ • 3层MLP     │    │ • LayerNorm  │          │
│  └──────────────┘    └──────────────┘    └──────────────┘          │
│         │                    │                    │                  │
│         ▼                    ▼                    ▼                  │
│  防止过平滑            防止塌缩            鼓励多样性                │
│                                                                       │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│  2. 训练流程层 (Training Process Level)                              │
├─────────────────────────────────────────────────────────────────────┤
│                                                                       │
│  Episode 0-1000        Episode 1000-3000       Episode 3000+         │
│  ┌──────────────┐     ┌──────────────┐       ┌──────────────┐      │
│  │   Warmup     │────▶│  Transition  │──────▶│  Fine-tune   │      │
│  │              │     │              │       │              │      │
│  │ • Softmax    │     │ • 温度退火   │       │ • Sparsemax  │      │
│  │ • LR=1e-4    │     │ • LR=5e-5    │       │ • LR=1e-5    │      │
│  │ • 高负载均衡 │     │ • 中负载均衡 │       │ • 低负载均衡 │      │
│  └──────────────┘     └──────────────┘       └──────────────┘      │
│         │                    │                       │               │
│         ▼                    ▼                       ▼               │
│  学基础策略            形成专家分工            精细调整              │
│                                                                       │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│  3. 损失函数层 (Loss Function Level)                                 │
├─────────────────────────────────────────────────────────────────────┤
│                                                                       │
│  Total Loss = Actor + Critic + Auxiliary                             │
│                                                                       │
│  ┌────────────────────────────────────────────────────────────┐     │
│  │  Actor Loss (PPO)                                          │     │
│  │  • Clipped Surrogate: min(r·A, clip(r)·A)                 │     │
│  │  • Entropy Bonus: -0.01 * H(π)                            │     │
│  └────────────────────────────────────────────────────────────┘     │
│                              +                                        │
│  ┌────────────────────────────────────────────────────────────┐     │
│  │  Critic Loss (MSE)                                         │     │
│  │  • Value Prediction: 0.5 * (V - R)²                       │     │
│  └────────────────────────────────────────────────────────────┘     │
│                              +                                        │
│  ┌────────────────────────────────────────────────────────────┐     │
│  │  Auxiliary Losses                                          │     │
│  │  • Load Balance: 0.01 * ||usage - uniform||²             │     │
│  │  • Diversity: 0.01 * CosineSim(experts)                  │     │
│  │  • Alpha Entropy: -0.05 * H(α)                           │     │
│  └────────────────────────────────────────────────────────────┘     │
│                                                                       │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│  4. 数值稳定性层 (Numerical Stability Level)                         │
├─────────────────────────────────────────────────────────────────────┤
│                                                                       │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐          │
│  │  NaN检测     │───▶│  梯度裁剪    │───▶│  奖励归一化  │          │
│  │              │    │              │    │              │          │
│  │ • 前向检查   │    │ • max_norm=1 │    │ • 滑动均值   │          │
│  │ • 损失检查   │    │ • 范数监控   │    │ • 标准差归一 │          │
│  │ • 自动回滚   │    │ • 逐层裁剪   │    │ • 裁剪到±10  │          │
│  └──────────────┘    └──────────────┘    └──────────────┘          │
│         │                    │                    │                  │
│         ▼                    ▼                    ▼                  │
│  防止崩溃              防止爆炸            稳定训练                  │
│                                                                       │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│  5. 监控诊断层 (Monitoring & Diagnosis Level)                        │
├─────────────────────────────────────────────────────────────────────┤
│                                                                       │
│  实时监控指标:                                                        │
│  ┌────────────────────────────────────────────────────────────┐     │
│  │  • episode_score          ↗ 逐渐上升                      │     │
│  │  • alpha_entropy          ↘ 0.5-1.0 (稳定)                │     │
│  │  • expert_usage           ≈ 均衡 (10-40% each)            │     │
│  │  • gradient_norm          < 5.0 (正常)                    │     │
│  │  • gat_attention_variance > 0.1 (动态)                    │     │
│  │  • operator_activation    10-30% (合理)                   │     │
│  └────────────────────────────────────────────────────────────┘     │
│                                                                       │
│  异常检测:                                                            │
│  ┌────────────────────────────────────────────────────────────┐     │
│  │  ⚠️ alpha_entropy < 0.3    → 专家塌缩                     │     │
│  │  ⚠️ gradient_norm > 10.0   → 梯度爆炸                     │     │
│  │  ⚠️ attention_var < 0.05   → GAT过平滑                    │     │
│  │  ⚠️ score不变              → 奖励问题                     │     │
│  └────────────────────────────────────────────────────────────┘     │
│                                                                       │
└─────────────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────────────┐
│  6. 降级方案层 (Fallback Strategy Level)                             │
├─────────────────────────────────────────────────────────────────────┤
│                                                                       │
│  如果训练失败，按顺序尝试:                                            │
│                                                                       │
│  ┌──────────────┐    ┌──────────────┐    ┌──────────────┐          │
│  │  方案1       │───▶│  方案2       │───▶│  方案3       │          │
│  │  固定GAT     │    │  用Softmax   │    │  减少专家    │          │
│  │              │    │              │    │              │          │
│  │ 冻结GAT参数  │    │ 禁用Sparsemax│   │ 4→2个专家    │          │
│  │ 只训练路由   │    │ 全程Softmax  │    │ 简化架构     │          │
│  └──────────────┘    └──────────────┘    └──────────────┘          │
│         │                    │                    │                  │
│         ▼                    ▼                    ▼                  │
│  如果仍失败 ────────────────────────────────────▶ 方案4              │
│                                                   回退V2+GAT         │
│                                                                       │
└─────────────────────────────────────────────────────────────────────┘

═══════════════════════════════════════════════════════════════════════
                          关键设计原则
═══════════════════════════════════════════════════════════════════════

1. 渐进式训练 (Progressive Training)
   Warmup → Transition → Fine-tune
   避免过早塌缩，平滑过渡到稀疏路由

2. 多重正则化 (Multiple Regularization)
   负载均衡 + 多样性 + 熵正则
   从多个角度防止专家塌缩

3. 严格监控 (Strict Monitoring)
   实时检测异常，及时干预
   记录所有关键指标，便于诊断

4. 数值稳定 (Numerical Stability)
   NaN检测 + 梯度裁剪 + 奖励归一化
   防止训练崩溃

5. 降级准备 (Fallback Ready)
   多个备选方案，避免全盘失败
   保证至少能达到V2水平

═══════════════════════════════════════════════════════════════════════
```

