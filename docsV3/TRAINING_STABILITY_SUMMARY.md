# V3 è®­ç»ƒç¨³å®šæ€§æªæ–½ - å¿«é€Ÿå‚è€ƒ

> **å®Œæ•´æ–‡æ¡£**: `V3_TRAINING_STABILITY_CHECKLIST.md`  
> **ç›®çš„**: å¿«é€ŸæŸ¥é˜…å…³é”®ç¨³å®šæ€§æªæ–½

---

## ğŸ¯ æ ¸å¿ƒé—®é¢˜

| é—®é¢˜ | ç—‡çŠ¶ | è§£å†³æ–¹æ¡ˆ |
|------|------|----------|
| **ä¸“å®¶å¡Œç¼©** | Î±ç†µ<0.3, æŸä¸“å®¶>80% | Warmup + è´Ÿè½½å‡è¡¡æŸå¤± + æ¸©åº¦é€€ç« |
| **GATè¿‡å¹³æ»‘** | æ³¨æ„åŠ›æ–¹å·®<0.05 | é™åˆ¶2å±‚ + æ®‹å·®è¿æ¥ + Dropout |
| **æ¢¯åº¦çˆ†ç‚¸** | æ¢¯åº¦èŒƒæ•°>10.0 | å°å­¦ä¹ ç‡ + æ¢¯åº¦è£å‰ª + LayerNorm |
| **å¥–åŠ±ä¸æ”¶æ•›** | Scoreé•¿æœŸä¸å˜ | å¥–åŠ±å¡‘å½¢ + ç†µæ­£åˆ™ + å¢åŠ æ¢ç´¢ |

---

## ğŸ“Š å…³é”®è¶…å‚æ•°

```python
# å­¦ä¹ ç‡
learning_rate = 1e-4  # V2æ˜¯3e-4ï¼ŒV3æ›´å°
warmup_steps = 1000
lr_scheduler = CosineAnnealingLR

# PPO
clip_ratio = 0.15     # V2æ˜¯0.2ï¼ŒV3æ›´ä¿å®ˆ
batch_size = 256      # V2æ˜¯128ï¼ŒV3æ›´å¤§
ppo_epochs = 4        # V2æ˜¯3ï¼ŒV3æ›´å……åˆ†

# æ­£åˆ™åŒ–
entropy_coef = 0.01           # åŠ¨ä½œç†µ
alpha_entropy_coef = 0.05     # ä¸“å®¶ç†µ
load_balance_coef = 0.01      # è´Ÿè½½å‡è¡¡
diversity_coef = 0.01         # ä¸“å®¶å¤šæ ·æ€§

# æ¢¯åº¦
max_grad_norm = 1.0   # V2æ˜¯0.5ï¼ŒV3æ›´å®½æ¾
```

---

## ğŸ”„ ä¸‰é˜¶æ®µè®­ç»ƒ

| é˜¶æ®µ | Episodes | è·¯ç”±æ–¹å¼ | å­¦ä¹ ç‡ | ç›®çš„ |
|------|----------|----------|--------|------|
| **Warmup** | 0-1000 | Softmax | 1e-4 | è®©ä¸“å®¶å­¦åŸºç¡€ç­–ç•¥ |
| **Transition** | 1000-3000 | æ¸©åº¦é€€ç« | 5e-5 | å¹³æ»‘è¿‡æ¸¡åˆ°ç¨€ç– |
| **Fine-tune** | 3000+ | Sparsemax | 1e-5 | ç²¾ç»†è°ƒæ•´åˆ†å·¥ |

```python
def get_training_config(episode):
    if episode < 1000:
        return {'use_sparsemax': False, 'lr': 1e-4}
    elif episode < 3000:
        temp = 1.0 - 0.5 * (episode - 1000) / 2000
        return {'use_sparsemax': True, 'temp': temp, 'lr': 5e-5}
    else:
        return {'use_sparsemax': True, 'temp': 0.5, 'lr': 1e-5}
```

---

## ğŸ›¡ï¸ å¿…é¡»å®ç°çš„è¾…åŠ©æŸå¤±

### 1. è´Ÿè½½å‡è¡¡æŸå¤± (é˜²æ­¢å¡Œç¼©)

```python
def load_balance_loss(alpha):
    expert_usage = alpha.mean(dim=0)
    target = torch.ones_like(expert_usage) / num_experts
    return F.mse_loss(expert_usage, target)
```

### 2. ä¸“å®¶å¤šæ ·æ€§æŸå¤± (é¼“åŠ±å·®å¼‚åŒ–)

```python
def expert_diversity_loss(expert_logits):
    # æœ€å°åŒ–ä¸“å®¶é—´ä½™å¼¦ç›¸ä¼¼åº¦
    num_experts = expert_logits.size(1)
    diversity = 0.0
    for i in range(num_experts):
        for j in range(i+1, num_experts):
            cos_sim = F.cosine_similarity(
                expert_logits[:, i, :], 
                expert_logits[:, j, :], 
                dim=-1
            ).mean()
            diversity += cos_sim
    return diversity / (num_experts * (num_experts - 1) / 2)
```

### 3. æ€»æŸå¤±ç»„åˆ

```python
total_loss = (
    actor_loss + 
    0.5 * critic_loss + 
    0.01 * load_balance_loss(alpha) +
    0.01 * expert_diversity_loss(expert_logits) -
    0.01 * entropy -
    0.05 * alpha_entropy
)
```

---

## ğŸ“ˆ ç›‘æ§æŒ‡æ ‡

### å¿…é¡»ç›‘æ§

| æŒ‡æ ‡ | æ­£å¸¸èŒƒå›´ | å¼‚å¸¸é˜ˆå€¼ |
|------|----------|----------|
| **alpha_entropy** | 0.5-1.0 | <0.3 æˆ– >1.2 |
| **expert_usage** | æ¯ä¸ª10-40% | æŸä¸ª>80% |
| **gradient_norm** | <5.0 | >10.0 |
| **gat_attention_variance** | >0.1 | <0.05 |

### ç›‘æ§ä»£ç 

```python
class TrainingMonitor:
    def check_anomalies(self, metrics):
        if metrics['alpha_entropy'] < 0.3:
            print("âš ï¸ ä¸“å®¶å¡Œç¼©!")
        if metrics['gradient_norm'] > 10.0:
            print("âš ï¸ æ¢¯åº¦çˆ†ç‚¸!")
        if metrics['gat_attention_variance'] < 0.05:
            print("âš ï¸ GATè¿‡å¹³æ»‘!")
```

---

## ğŸ”§ æ•°å€¼ç¨³å®šæ€§

### NaN/Infå¤„ç†

```python
# åœ¨forwardä¸­
logits = torch.nan_to_num(logits, nan=0.0, posinf=20.0, neginf=-20.0)
logits = logits.clamp(-20.0, 20.0)

# åœ¨updateä¸­
if torch.isnan(loss) or torch.isinf(loss):
    print("âš ï¸ NaN detected! Skipping batch...")
    continue
```

### æ¢¯åº¦è£å‰ª

```python
nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0)
```

### å¥–åŠ±å½’ä¸€åŒ–

```python
class RewardNormalizer:
    def normalize(self, reward):
        normalized = (reward - self.mean) / (self.std + 1e-8)
        return np.clip(normalized, -10.0, 10.0)
```

---

## ğŸš¨ é™çº§æ–¹æ¡ˆ

å¦‚æœè®­ç»ƒå¤±è´¥ï¼ŒæŒ‰é¡ºåºå°è¯•ï¼š

1. **å›ºå®šGAT**: å†»ç»“GATå‚æ•°ï¼Œåªè®­ç»ƒè·¯ç”±å’Œä¸“å®¶
2. **ä½¿ç”¨Softmax**: ç¦ç”¨Sparsemaxï¼Œå…¨ç¨‹ä½¿ç”¨Softmax
3. **å‡å°‘ä¸“å®¶**: ä»4ä¸ªå‡å°‘åˆ°2ä¸ª (Survival + General)
4. **å›é€€V2+GAT**: ç”¨GATæå–ç‰¹å¾ï¼Œä½†ç”¨V2è·¯ç”±

---

## âœ… å®æ–½å‰æ£€æŸ¥æ¸…å•

- [ ] å­¦ä¹ ç‡è®¾ä¸º1e-4 (æ¯”V2å°)
- [ ] å®ç°è´Ÿè½½å‡è¡¡æŸå¤±
- [ ] å®ç°ä¸“å®¶å¤šæ ·æ€§æŸå¤±
- [ ] å®ç°Warmupæœºåˆ¶ (Softmax â†’ Sparsemax)
- [ ] å®ç°æ¸©åº¦é€€ç«
- [ ] å®ç°NaNæ£€æµ‹å’Œå›æ»š
- [ ] å®ç°TrainingMonitor
- [ ] æ¢¯åº¦è£å‰ªmax_norm=1.0
- [ ] æ‰€æœ‰logitsåšnan_to_numå’Œclamp
- [ ] æ¯100 episodesä¿å­˜checkpoint

---

## ğŸ“š å®Œæ•´æ–‡æ¡£

è¯¦ç»†è¯´æ˜è¯·å‚è€ƒ: `docsV3/V3_TRAINING_STABILITY_CHECKLIST.md`

