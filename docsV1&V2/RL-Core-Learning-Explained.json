{
  "RL最后学的是什么": {
    "一句话": "RL学的不是'选哪个动作'，而是学'在什么情景下，相信哪个信息通路'",
    "具体例子": {
      "场景1_高置信": {
        "状态": "你在迷宫里看到楼梯，非常清晰明确",
        "各通路的一致性": {
          "q_pre": "有金币 + 不混乱 + 力量满",
          "q_scene": "在安全区域 + 目标清晰",
          "q_effect": "下楼的成功率0.95，很安全",
          "q_rule": "没有陷阱"
        },
        "四个通路说": "都说走",
        "RL学到的alpha": "[0.25, 0.25, 0.25, 0.25]",
        "RL做出的决策": "直接执行move，0 token，微秒级",
        "confidence": ">= 0.78"
      },
      "场景2_低置信_各通路分歧": {
        "状态": "你在商店里看到一个奇怪的物体，不知道是什么",
        "各通路的意见": {
          "q_pre": "库存满了，不能捡东西",
          "q_scene": "在商店里，通常安全",
          "q_effect": "不知道吃了会怎样，安全性0.4很低",
          "q_rule": "未知物体，可能有cursed"
        },
        "四个通路说": "意见不统一",
        "RL学到的alpha": "[0.15, 0.25, 0.3, 0.3]",
        "含义": "RL学到在这种情景，效果和规则风险很高",
        "RL做出的决策": "触发QUERY模式，询问这个物体是什么",
        "confidence": "< 0.78"
      }
    },
    "本质": {
      "不是trade": "RL不是在trade进度vs安全，而是在判断信息的可靠度",
      "而是边界": "RL学的是在什么情景下用什么策略的边界"
    }
  },
  "RL的动作是什么": {
    "高置信分支": {
      "条件": "confidence >= 0.78",
      "动作空间": "6-10个原始游戏动作",
      "动作例子": "move, eat, zap, wait, unlock_door",
      "从谁选": "从超图给出的高置信匹配子图里",
      "怎么选": "RL根据alpha权重，融合4个actor的logits，用softmax采样"
    },
    "低置信分支": {
      "条件": "confidence < 0.78",
      "动作空间": "查询和探索动作",
      "动作1_query_property": {
        "含义": "询问LLM或规则库这个物体是什么，有什么属性",
        "代价": "100-500 tokens"
      },
      "动作2_safe_exploration": {
        "含义": "做标记为安全的探测，如detect、identify",
        "代价": "0 tokens，纯游戏内动作"
      },
      "动作3_llm_reflection": {
        "含义": "把整个情景交给LLM做长思维链反思",
        "代价": "1000+ tokens"
      },
      "动作4_wait": {
        "含义": "等待更多信息，让怪物移动、时间流逝reveal新信息",
        "代价": "1步时间，可能有风险"
      }
    },
    "关键": "高置信时选游戏动作，低置信时选查询策略，都是RL在选"
  },
  "查询谁": {
    "query_property_查询LLM": {
      "查询对象": "LLM作为符号知识库和分析工具",
      "流程": [
        "1. RL选择query_property动作",
        "2. 系统打包当前状态给LLM",
        "3. LLM返回物体的可能属性和含义",
        "4. 系统更新belief和evidence graph",
        "5. confidence提高，可能进入RL_FAST分支"
      ]
    },
    "safe_exploration_查询超图和游戏": {
      "查询对象": "游戏规则和超图标记的安全动作",
      "流程": [
        "1. RL选择safe_exploration动作",
        "2. 系统检查超图中标记为safe的detection动作",
        "3. 执行这些检测（detect_monster, identify, ...）",
        "4. 获得返回信息",
        "5. confidence提高"
      ]
    },
    "llm_reflection_查询LLM": {
      "查询对象": "LLM做完整的策略反思",
      "流程": [
        "1. RL选择llm_reflection动作",
        "2. 系统发送完整当前情景给LLM",
        "3. LLM做长思维链策略反思",
        "4. LLM返回高级策略指导",
        "5. 后续RL决策受到指导"
      ]
    },
    "wait_查询时间": {
      "查询对象": "时间让环境提供更多信息",
      "流程": [
        "1. RL选择wait动作",
        "2. 游戏时间流逝1步",
        "3. 怪物可能移动，视野信息可能变化",
        "4. belief更新，confidence可能提高"
      ]
    }
  },
  "整个流程的真实例子": {
    "场景": "L5房间，hp 60%，看到黄色卷轴，不知道是什么",
    "初始confidence": "0.62 (小于0.78，低置信)",
    "步骤1_计算alpha": {
      "q_pre说": "库存有空间，不混乱，可以拿",
      "q_scene说": "在商店，通常安全，但未知物体需谨慎",
      "q_effect说": "不知道吃卷轴会怎样，安全性未知",
      "q_rule说": "卷轴有很多conditional_effects，如传送、混乱、诅咒",
      "结果": "alpha = [0.2, 0.25, 0.3, 0.25]"
    },
    "步骤2_进入QUERY分支": {
      "原因": "confidence < 0.78",
      "候选": "query_property, safe_exploration, llm_reflection, wait"
    },
    "步骤3_RL选择动作": {
      "选择": "query_property",
      "原因": "alpha_rule和alpha_effect都很高，最需要了解属性"
    },
    "步骤4_执行query": {
      "给LLM信息": "你在L5商店，看到黄色卷轴，不知道是什么",
      "LLM返回": "黄色卷轴最可能是XXX，属性包括...",
      "系统更新": "belief和evidence graph"
    },
    "步骤5_下一步": {
      "新confidence": "0.85，现在知道了卷轴属性",
      "进入": "RL_FAST分支，可以直接选吃或不吃"
    }
  }
}