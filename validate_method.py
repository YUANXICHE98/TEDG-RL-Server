#!/usr/bin/env python3
"""éªŒè¯æ–¹æ³•æœ‰æ•ˆæ€§ - åˆ†æè®­ç»ƒç»“æœ"""

import os
import sys
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import json
import numpy as np
from pathlib import Path
import matplotlib.pyplot as plt
from scipy.stats import linregress


def load_training_log():
    """åŠ è½½è®­ç»ƒæ—¥å¿—"""
    log_path = Path("results/logs/training_log.json")
    if not log_path.exists():
        print("âŒ è®­ç»ƒæ—¥å¿—ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒ")
        return None
    
    with open(log_path, 'r') as f:
        return json.load(f)


def validate_alpha_distribution(log):
    """éªŒè¯1: Î±æƒé‡åˆ†å¸ƒåˆç†æ€§"""
    print("\n" + "="*80)
    print("éªŒè¯1: Î±æƒé‡åˆ†å¸ƒ")
    print("="*80)
    
    alpha_history = np.array(log['alpha_history'])
    alpha_mean = alpha_history.mean(axis=0)
    alpha_std = alpha_history.std(axis=0)
    
    print(f"\nÎ±æƒé‡ç»Ÿè®¡:")
    print(f"  Î±_pre:    {alpha_mean[0]:.3f} Â± {alpha_std[0]:.3f}")
    print(f"  Î±_scene:  {alpha_mean[1]:.3f} Â± {alpha_std[1]:.3f}")
    print(f"  Î±_effect: {alpha_mean[2]:.3f} Â± {alpha_std[2]:.3f}")
    print(f"  Î±_rule:   {alpha_mean[3]:.3f} Â± {alpha_std[3]:.3f}")
    
    # æ£€æŸ¥1: å‡å€¼åœ¨åˆç†èŒƒå›´
    checks = []
    for i, name in enumerate(['pre', 'scene', 'effect', 'rule']):
        in_range = 0.15 < alpha_mean[i] < 0.35
        checks.append(in_range)
        status = "âœ“" if in_range else "âœ—"
        print(f"\n{status} Î±_{name}å‡å€¼åœ¨[0.15, 0.35]: {in_range}")
    
    # æ£€æŸ¥2: æœ‰åŠ¨æ€å˜åŒ–
    has_variance = all(std > 0.05 for std in alpha_std)
    checks.append(has_variance)
    print(f"\n{'âœ“' if has_variance else 'âœ—'} Î±æƒé‡æœ‰åŠ¨æ€å˜åŒ– (std > 0.05): {has_variance}")
    
    # æ£€æŸ¥3: ä¸è¿‡åº¦é›†ä¸­
    max_mean = alpha_mean.max()
    min_mean = alpha_mean.min()
    not_concentrated = (max_mean - min_mean) < 0.2
    checks.append(not_concentrated)
    print(f"\n{'âœ“' if not_concentrated else 'âœ—'} Î±æƒé‡ä¸è¿‡åº¦é›†ä¸­ (å·®å¼‚ < 0.2): {not_concentrated}")
    
    success_rate = sum(checks) / len(checks) * 100
    print(f"\næ€»ä½“: {sum(checks)}/{len(checks)} é€šè¿‡ ({success_rate:.0f}%)")
    
    return all(checks)


def validate_performance_improvement(log):
    """éªŒè¯2: æ€§èƒ½æå‡"""
    print("\n" + "="*80)
    print("éªŒè¯2: æ€§èƒ½æå‡")
    print("="*80)
    
    rewards = log['episode_rewards']
    scores = log['episode_scores']
    lengths = log['episode_lengths']
    
    # åˆæœŸ vs åæœŸ
    early_rewards = np.mean(rewards[:100])
    late_rewards = np.mean(rewards[-100:])
    early_scores = np.mean(scores[:100])
    late_scores = np.mean(scores[-100:])
    early_lengths = np.mean(lengths[:100])
    late_lengths = np.mean(lengths[-100:])
    
    print(f"\nåˆæœŸ vs åæœŸ:")
    print(f"  å¥–åŠ±:  {early_rewards:.2f} â†’ {late_rewards:.2f} ({late_rewards/max(early_rewards,0.1):.2f}x)")
    print(f"  åˆ†æ•°:  {early_scores:.0f} â†’ {late_scores:.0f} ({late_scores/max(early_scores,1):.2f}x)")
    print(f"  é•¿åº¦:  {early_lengths:.0f} â†’ {late_lengths:.0f} ({late_lengths/max(early_lengths,1):.2f}x)")
    
    # è¶‹åŠ¿æ£€æŸ¥
    checks = []
    
    # æ£€æŸ¥1: å¥–åŠ±ä¸Šå‡
    slope, _, _, p_value, _ = linregress(range(len(rewards)), rewards)
    reward_improving = slope > 0 and p_value < 0.05
    checks.append(reward_improving)
    print(f"\n{'âœ“' if reward_improving else 'âœ—'} å¥–åŠ±æ˜¾è‘—ä¸Šå‡ (slope={slope:.6f}, p={p_value:.4f}): {reward_improving}")
    
    # æ£€æŸ¥2: åˆ†æ•°æå‡
    score_improvement = late_scores > early_scores * 1.5
    checks.append(score_improvement)
    print(f"\n{'âœ“' if score_improvement else 'âœ—'} åˆ†æ•°æå‡ > 50%: {score_improvement}")
    
    # æ£€æŸ¥3: é•¿åº¦å¢åŠ 
    length_improvement = late_lengths > early_lengths * 1.5
    checks.append(length_improvement)
    print(f"\n{'âœ“' if length_improvement else 'âœ—'} Episodeé•¿åº¦å¢åŠ  > 50%: {length_improvement}")
    
    success_rate = sum(checks) / len(checks) * 100
    print(f"\næ€»ä½“: {sum(checks)}/{len(checks)} é€šè¿‡ ({success_rate:.0f}%)")
    
    return all(checks)


def validate_behavior_rationality(log):
    """éªŒè¯3: è¡Œä¸ºåˆç†æ€§ï¼ˆç†è®ºéªŒè¯ï¼‰"""
    print("\n" + "="*80)
    print("éªŒè¯3: è¡Œä¸ºåˆç†æ€§")
    print("="*80)
    
    alpha_history = np.array(log['alpha_history'])
    
    print("\nç†è®ºéªŒè¯: Î±æƒé‡åº”è¯¥æ ¹æ®åœºæ™¯å˜åŒ–")
    
    # æ£€æŸ¥Î±æƒé‡çš„å˜åŒ–èŒƒå›´
    alpha_ranges = []
    for i, name in enumerate(['pre', 'scene', 'effect', 'rule']):
        min_val = alpha_history[:, i].min()
        max_val = alpha_history[:, i].max()
        range_val = max_val - min_val
        alpha_ranges.append(range_val)
        print(f"  Î±_{name}: èŒƒå›´ [{min_val:.3f}, {max_val:.3f}], å˜åŒ–å¹…åº¦ {range_val:.3f}")
    
    # æ£€æŸ¥: æ¯ä¸ªÎ±éƒ½æœ‰æ˜¾è‘—å˜åŒ–
    checks = []
    significant_variation = all(r > 0.1 for r in alpha_ranges)
    checks.append(significant_variation)
    print(f"\n{'âœ“' if significant_variation else 'âœ—'} æ‰€æœ‰Î±æƒé‡éƒ½æœ‰æ˜¾è‘—å˜åŒ– (> 0.1): {significant_variation}")
    
    # æ£€æŸ¥: Î±æƒé‡ä¸æ˜¯å›ºå®šçš„
    alpha_std_over_time = alpha_history.std(axis=0).mean()
    not_fixed = alpha_std_over_time > 0.05
    checks.append(not_fixed)
    print(f"\n{'âœ“' if not_fixed else 'âœ—'} Î±æƒé‡åŠ¨æ€å˜åŒ– (å¹³å‡std={alpha_std_over_time:.3f}): {not_fixed}")
    
    success_rate = sum(checks) / len(checks) * 100
    print(f"\næ€»ä½“: {sum(checks)}/{len(checks)} é€šè¿‡ ({success_rate:.0f}%)")
    
    return all(checks)


def generate_visualizations(log):
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
    print("\n" + "="*80)
    print("ç”Ÿæˆå¯è§†åŒ–")
    print("="*80)
    
    output_dir = Path("results/validation")
    output_dir.mkdir(exist_ok=True)
    
    # å›¾1: æ€§èƒ½æ›²çº¿
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # å¥–åŠ±æ›²çº¿
    rewards = log['episode_rewards']
    window = 50
    rewards_smooth = np.convolve(rewards, np.ones(window)/window, mode='valid')
    axes[0, 0].plot(rewards, alpha=0.3, label='Raw')
    axes[0, 0].plot(range(window-1, len(rewards)), rewards_smooth, label=f'Smooth ({window})')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Reward')
    axes[0, 0].set_title('è®­ç»ƒå¥–åŠ±æ›²çº¿')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # åˆ†æ•°æ›²çº¿
    scores = log['episode_scores']
    scores_smooth = np.convolve(scores, np.ones(window)/window, mode='valid')
    axes[0, 1].plot(scores, alpha=0.3, label='Raw')
    axes[0, 1].plot(range(window-1, len(scores)), scores_smooth, label=f'Smooth ({window})')
    axes[0, 1].set_xlabel('Episode')
    axes[0, 1].set_ylabel('Score')
    axes[0, 1].set_title('NetHackåˆ†æ•°æ›²çº¿')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # Episodeé•¿åº¦
    lengths = log['episode_lengths']
    lengths_smooth = np.convolve(lengths, np.ones(window)/window, mode='valid')
    axes[1, 0].plot(lengths, alpha=0.3, label='Raw')
    axes[1, 0].plot(range(window-1, len(lengths)), lengths_smooth, label=f'Smooth ({window})')
    axes[1, 0].set_xlabel('Episode')
    axes[1, 0].set_ylabel('Steps')
    axes[1, 0].set_title('Episodeé•¿åº¦æ›²çº¿')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # Î±æƒé‡åˆ†å¸ƒ
    alpha_history = np.array(log['alpha_history'])
    for i, name in enumerate(['Î±_pre', 'Î±_scene', 'Î±_effect', 'Î±_rule']):
        axes[1, 1].plot(alpha_history[:, i], label=name, alpha=0.7)
    axes[1, 1].set_xlabel('Episode')
    axes[1, 1].set_ylabel('Î± Weight')
    axes[1, 1].set_title('Î±æƒé‡å˜åŒ–')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(output_dir / 'training_curves.png', dpi=150)
    print(f"âœ“ ä¿å­˜: {output_dir / 'training_curves.png'}")
    
    # å›¾2: Î±æƒé‡åˆ†å¸ƒç®±çº¿å›¾
    fig, ax = plt.subplots(figsize=(10, 6))
    alpha_data = [alpha_history[:, i] for i in range(4)]
    ax.boxplot(alpha_data, labels=['Î±_pre', 'Î±_scene', 'Î±_effect', 'Î±_rule'])
    ax.set_ylabel('Î± Weight')
    ax.set_title('Î±æƒé‡åˆ†å¸ƒ')
    ax.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.savefig(output_dir / 'alpha_distribution.png', dpi=150)
    print(f"âœ“ ä¿å­˜: {output_dir / 'alpha_distribution.png'}")
    
    plt.close('all')


def main():
    """ä¸»éªŒè¯æµç¨‹"""
    print("="*80)
    print("TEDG-RL æ–¹æ³•æœ‰æ•ˆæ€§éªŒè¯")
    print("="*80)
    
    # åŠ è½½æ—¥å¿—
    log = load_training_log()
    if log is None:
        return
    
    print(f"\nè®­ç»ƒä¿¡æ¯:")
    print(f"  Episodes: {log['total_episodes']}")
    print(f"  è®­ç»ƒæ—¶é—´: {log['total_time_seconds']/60:.1f}åˆ†é’Ÿ")
    print(f"  è®¾å¤‡: {log['device']}")
    print(f"  æœ€ä½³å¥–åŠ±: {log['best_reward']:.2f}")
    print(f"  æœ€ä½³åˆ†æ•°: {log['best_score']:.0f}")
    
    # æ‰§è¡ŒéªŒè¯
    results = []
    
    results.append(("Î±æƒé‡åˆ†å¸ƒ", validate_alpha_distribution(log)))
    results.append(("æ€§èƒ½æå‡", validate_performance_improvement(log)))
    results.append(("è¡Œä¸ºåˆç†æ€§", validate_behavior_rationality(log)))
    
    # ç”Ÿæˆå¯è§†åŒ–
    try:
        generate_visualizations(log)
    except Exception as e:
        print(f"\nâš  å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("éªŒè¯æ€»ç»“")
    print("="*80)
    
    for name, passed in results:
        status = "âœ“ é€šè¿‡" if passed else "âœ— æœªé€šè¿‡"
        print(f"{status}: {name}")
    
    total_passed = sum(1 for _, p in results if p)
    success_rate = total_passed / len(results) * 100
    
    print(f"\næ€»ä½“: {total_passed}/{len(results)} é€šè¿‡ ({success_rate:.0f}%)")
    
    if success_rate >= 66:
        print("\nğŸ‰ æ–¹æ³•æœ‰æ•ˆæ€§éªŒè¯é€šè¿‡ï¼")
        print("\nè®ºæ–‡ä¸­å¯ä»¥è¯´æ˜:")
        print("  1. Î±æƒé‡å­¦ä¹ åˆç†ï¼ˆå‡è¡¡åˆ†å¸ƒï¼ŒåŠ¨æ€å˜åŒ–ï¼‰")
        print("  2. æ€§èƒ½æ˜¾è‘—æå‡ï¼ˆå¥–åŠ±/åˆ†æ•°/é•¿åº¦éƒ½æå‡ï¼‰")
        print("  3. è¡Œä¸ºå…·æœ‰åœºæ™¯é€‚åº”æ€§ï¼ˆÎ±æƒé‡æ ¹æ®æƒ…å†µå˜åŒ–ï¼‰")
    else:
        print("\nâš  éƒ¨åˆ†éªŒè¯æœªé€šè¿‡ï¼Œå»ºè®®:")
        print("  1. å¢åŠ è®­ç»ƒepisodes")
        print("  2. è°ƒæ•´è¶…å‚æ•°")
        print("  3. æ£€æŸ¥å¥–åŠ±å‡½æ•°è®¾è®¡")


if __name__ == "__main__":
    main()
