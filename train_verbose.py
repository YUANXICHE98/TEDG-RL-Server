#!/usr/bin/env python3
"""TEDG-RL NetHackè®­ç»ƒ - è¯¦ç»†ç›‘æ§ç‰ˆæœ¬"""

import os
import sys
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import torch
import numpy as np
from pathlib import Path
import json
from datetime import datetime
from tqdm import tqdm
import time
import gymnasium as gym
import nle.env
import nle.nethack as nh

from src.core.state_constructor import StateConstructor
from src.core.networks_correct import MultiChannelPolicyNet
from src.core.ppo_trainer import PPOTrainer
from src.core.action_masking import ActionMasker


def print_section(title):
    """æ‰“å°åˆ†èŠ‚æ ‡é¢˜"""
    print(f"\n{'='*80}")
    print(f"  {title}")
    print(f"{'='*80}")


def print_step(step, content):
    """æ‰“å°æ­¥éª¤"""
    print(f"[æ­¥éª¤{step}] {content}")


def get_device():
    """æ£€æµ‹è®¾å¤‡"""
    try:
        import torch_musa
        if torch.musa.is_available():
            device = torch.device('musa:0')
            print(f"âœ“ MUSA GPU: {torch.musa.get_device_name(0)}")
            print(f"  æ˜¾å­˜: {torch.musa.get_device_properties(0).total_memory / 1024**3:.2f} GB")
            return device
    except:
        pass
    
    if torch.cuda.is_available():
        device = torch.device('cuda:0')
        print(f"âœ“ CUDA GPU: {torch.cuda.get_device_name(0)}")
        return device
    
    print("âš  ä½¿ç”¨CPU")
    return torch.device('cpu')


def extract_state_from_nethack_obs(obs: dict, state_constructor: StateConstructor, verbose=False) -> np.ndarray:
    """ä»NetHackè§‚æµ‹æå–state"""
    blstats = obs.get('blstats', np.zeros(nh.NLE_BLSTATS_SIZE))
    
    if verbose:
        print(f"\n  [è§‚æµ‹è§£æ]")
        print(
            f"    HP: {int(blstats[nh.NLE_BL_HP])}/{int(blstats[nh.NLE_BL_HPMAX])} "
            f"({blstats[nh.NLE_BL_HP]/max(blstats[nh.NLE_BL_HPMAX],1)*100:.0f}%)"
        )
        print(f"    æ·±åº¦: {int(blstats[nh.NLE_BL_DEPTH])}å±‚")
        print(f"    é‡‘å¸: {int(blstats[nh.NLE_BL_GOLD])}")
        print(f"    é¥¥é¥¿: {int(blstats[nh.NLE_BL_HUNGER])}")
        print(f"    ä½ç½®: ({int(blstats[nh.NLE_BL_X])}, {int(blstats[nh.NLE_BL_Y])})")
        print(f"    åˆ†æ•°: {int(blstats[nh.NLE_BL_SCORE])}")
    
    # æ„é€ belief (50ç»´)
    belief = np.zeros(50, dtype=np.float32)
    belief[0] = blstats[nh.NLE_BL_HP] / max(blstats[nh.NLE_BL_HPMAX], 1)  # hp_ratio
    belief[1] = blstats[nh.NLE_BL_DEPTH] / 50.0  # depth
    belief[2] = min(blstats[nh.NLE_BL_GOLD] / 1000.0, 1.0)  # gold
    belief[3] = blstats[nh.NLE_BL_HUNGER] / 1000.0  # hunger
    belief[4] = blstats[nh.NLE_BL_STR25] / 25.0  # strength
    belief[5] = blstats[nh.NLE_BL_DEX] / 25.0  # dex
    belief[6] = blstats[nh.NLE_BL_CON] / 25.0  # con
    belief[7] = blstats[nh.NLE_BL_INT] / 25.0  # int
    belief[8] = blstats[nh.NLE_BL_WIS] / 25.0  # wis
    belief[9] = blstats[nh.NLE_BL_CHA] / 25.0  # cha
    belief[10] = blstats[nh.NLE_BL_X] / 79.0  # x
    belief[11] = blstats[nh.NLE_BL_Y] / 21.0  # y
    belief[12] = blstats[nh.NLE_BL_SCORE] / 10000.0  # score
    belief[30] = 1.0 if blstats[nh.NLE_BL_HP] < blstats[nh.NLE_BL_HPMAX] * 0.3 else 0.0  # low_hp
    belief[31] = 1.0 if blstats[nh.NLE_BL_HUNGER] > 800 else 0.0  # hungry
    
    # ä»è¶…å›¾é€‰æ‹©åŒ¹é…çš„è¾¹
    edges = state_constructor.hypergraph['hyperedges']
    edge = np.random.choice(edges)
    
    # æ¨æ–­pre_nodes (å¢å¼ºç‰ˆ - æå–æ›´å¤šä¿¡æ¯)
    pre_nodes = []
    
    # HPçŠ¶æ€
    hp_ratio = blstats[nh.NLE_BL_HP] / max(blstats[nh.NLE_BL_HPMAX], 1)
    if hp_ratio >= 0.8:
        pre_nodes.append('hp_full')
    elif hp_ratio < 0.3:
        pre_nodes.append('hp_low')
    else:
        pre_nodes.append('hp_medium')
    
    # é¥¥é¥¿çŠ¶æ€
    if blstats[nh.NLE_BL_HUNGER] < 500:
        pre_nodes.append('hunger_normal')
    elif blstats[nh.NLE_BL_HUNGER] > 800:
        pre_nodes.append('hungry')
    
    # é‡‘å¸
    if blstats[nh.NLE_BL_GOLD] > 0:
        pre_nodes.append('has_gold')
    else:
        pre_nodes.append('no_gold')
    
    # åŠ›é‡
    if blstats[nh.NLE_BL_STR25] > 18:
        pre_nodes.append('strong')
    
    # è£…å¤‡çŠ¶æ€
    if blstats[nh.NLE_BL_AC] < 0:  # AC (armor class)
        pre_nodes.append('well_armored')
    elif blstats[nh.NLE_BL_AC] > 5:
        pre_nodes.append('poorly_armored')
    
    # scene_atoms (å¢å¼ºç‰ˆ - æå–æ›´å¤šåœºæ™¯ä¿¡æ¯)
    scene_atoms = []
    
    # 1. æ·±åº¦ä¿¡æ¯
    scene_atoms.append(f'dlvl_{int(blstats[nh.NLE_BL_DEPTH])}')
    
    # 2. ä½ç½®ä¿¡æ¯
    x, y = int(blstats[nh.NLE_BL_X]), int(blstats[nh.NLE_BL_Y])
    if x < 20:
        scene_atoms.append('near_left_edge')
    elif x > 60:
        scene_atoms.append('near_right_edge')
    
    if y < 5:
        scene_atoms.append('near_top')
    elif y > 16:
        scene_atoms.append('near_bottom')
    
    # 3. ACçŠ¶æ€ (é˜²å¾¡)
    ac = blstats[nh.NLE_BL_AC]
    if ac < 0:
        scene_atoms.append('ac_good')
    elif ac > 5:
        scene_atoms.append('ac_poor')
    
    # 4. ç»éªŒç­‰çº§
    exp_level = int(blstats[nh.NLE_BL_EXP])
    if exp_level <= 3:
        scene_atoms.append('exp_low')
    elif exp_level >= 10:
        scene_atoms.append('exp_high')
    else:
        scene_atoms.append(f'exp_{exp_level}')
    
    # 5. æ€ªç‰©æ£€æµ‹ (ç®€åŒ–ç‰ˆ - æ£€æŸ¥glyphs)
    glyphs = obs.get('glyphs', np.zeros((21, 79)))
    nearby_glyphs = glyphs[max(0,y-2):min(21,y+3), max(0,x-2):min(79,x+3)]
    # NetHackæ€ªç‰©çš„glyphèŒƒå›´å¤§çº¦æ˜¯381-638
    if np.any((nearby_glyphs >= 381) & (nearby_glyphs <= 638)):
        scene_atoms.append('monsters_present')
    
    eff_metadata = edge.get('eff_metadata', {})
    conditional_effects = eff_metadata.get('conditional_effects', [])
    
    confidence = 0.5 + 0.3 * (blstats[nh.NLE_BL_HP] / max(blstats[nh.NLE_BL_HPMAX], 1))
    
    if verbose:
        print(f"\n  [è¶…å›¾åŒ¹é…]")
        print(f"    å‰ç½®æ¡ä»¶: {pre_nodes[:3]}")
        print(f"    åœºæ™¯åŸå­: {scene_atoms}")
        print(f"    ç½®ä¿¡åº¦: {confidence:.2f}")
    
    goal = np.zeros(16, dtype=np.float32)
    goal[0] = 1.0
    
    state = state_constructor.construct_state(
        belief_vector=belief,
        pre_nodes=pre_nodes,
        scene_atoms=scene_atoms,
        eff_metadata=eff_metadata,
        conditional_effects=conditional_effects,
        confidence=confidence,
        goal_embedding=goal,
    )
    
    if verbose:
        print(f"\n  [çŠ¶æ€æ„é€ ]")
        print(f"    stateç»´åº¦: {state.shape}")
        print(f"    belief: {belief[:5]} ...")
        print(f"    q_pre: {state[50:55]} ...")
        print(f"    q_scene: {state[65:70]} ...")
        print(f"    q_effect: {state[80:85]} ...")
        print(f"    q_rule: {state[88:93]} ...")
    
    return state


def main():
    """ä¸»è®­ç»ƒå¾ªç¯"""
    print_section("TEDG-RL NetHackè®­ç»ƒ - è¯¦ç»†ç›‘æ§ç‰ˆ")
    
    # è®¾å¤‡æ£€æµ‹
    print("\n[åˆå§‹åŒ–]")
    print_step(1, "æ£€æµ‹è®¡ç®—è®¾å¤‡")
    device = get_device()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("results")
    output_dir.mkdir(exist_ok=True)
    (output_dir / "checkpoints").mkdir(exist_ok=True)
    (output_dir / "logs").mkdir(exist_ok=True)
    
    # åŠ è½½è¶…å›¾
    print_step(2, "åŠ è½½è¶…å›¾æ•°æ®")
    state_constructor = StateConstructor("data/hypergraph/hypergraph_complete_real.json")
    print(f"  âœ“ åŠ è½½å®Œæˆ")
    
    # åˆå§‹åŒ–åŠ¨ä½œæ©è”½
    print_step(3, "åˆå§‹åŒ–åŠ¨ä½œæ©è”½å™¨")
    action_masker = ActionMasker(state_constructor.hypergraph, num_actions=23)
    print(f"  âœ“ åˆå§‹åŒ–å®Œæˆ")
    
    # åˆ›å»ºNetHackç¯å¢ƒ
    print_step(4, "åˆ›å»ºNetHackç¯å¢ƒ")
    try:
        env = gym.make('NetHackScore-v0')
        print(f"  âœ“ NetHackScore-v0")
    except:
        env = gym.make('NetHack-v0')
        print(f"  âœ“ NetHack-v0")
    print(f"  åŠ¨ä½œç©ºé—´: {env.action_space.n}ä¸ªåŠ¨ä½œ")
    
    # åˆ›å»ºç½‘ç»œ
    print_step(5, "åˆå§‹åŒ–å¤šé€šé“ç­–ç•¥ç½‘ç»œ")
    policy_net = MultiChannelPolicyNet(
        state_dim=115,
        action_dim=23,
        actor_hidden_dim=128,
        attention_hidden_dim=64,
    )
    total_params = sum(p.numel() for p in policy_net.parameters())
    print(f"  âœ“ ç½‘ç»œå‚æ•°: {total_params:,}")
    print(f"  - 4ä¸ªç‹¬ç«‹Actor")
    print(f"  - 1ä¸ªAttentionWeightNet")
    print(f"  - 1ä¸ªå…±äº«Critic")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print_step(6, "åˆå§‹åŒ–PPOè®­ç»ƒå™¨")
    trainer = PPOTrainer(
        policy_net=policy_net,
        learning_rate=3e-4,
        clip_ratio=0.2,
        gamma=0.99,
        gae_lambda=0.95,
        ppo_epochs=3,
        batch_size=128,
        device=device,
    )
    print(f"  âœ“ è®­ç»ƒå™¨å°±ç»ª")
    print(f"  - å­¦ä¹ ç‡: 3e-4")
    print(f"  - Batch size: 128")
    print(f"  - PPO epochs: 3")
    
    # è®­ç»ƒå‚æ•°
    num_episodes = 10000
    max_steps = 1000
    eval_interval = 50  # æ›´é¢‘ç¹çš„è¯„ä¼°
    checkpoint_interval = 500
    verbose_interval = 10  # æ¯10ä¸ªepisodeè¯¦ç»†æ‰“å°ä¸€æ¬¡
    
    # ç»Ÿè®¡
    episode_rewards = []
    episode_lengths = []
    episode_scores = []
    alpha_history = []
    best_reward = float('-inf')
    best_score = 0
    
    start_time = time.time()
    
    print_section("å¼€å§‹è®­ç»ƒ")
    print(f"æ€»Episodes: {num_episodes}")
    print(f"æ¯Episodeæœ€å¤§æ­¥æ•°: {max_steps}")
    print(f"è®¾å¤‡: {device}")
    print(f"ç›®æ ‡: å­¦ä¹ Î±æƒé‡åŠ¨æ€åˆ†é… + æœ€å¤§åŒ–NetHackåˆ†æ•°")
    
    # ä¸»è®­ç»ƒå¾ªç¯
    for episode in range(num_episodes):
        verbose = (episode % verbose_interval == 0)  # æ¯10ä¸ªepisodeè¯¦ç»†æ‰“å°
        
        if verbose:
            print(f"\n{'â”€'*80}")
            print(f"Episode {episode+1}/{num_episodes}")
            print(f"{'â”€'*80}")
        
        # é‡ç½®ç¯å¢ƒ
        if verbose:
            print(f"\n[1. é‡ç½®NetHackç¯å¢ƒ]")
        obs, info = env.reset()
        state = extract_state_from_nethack_obs(obs, state_constructor, verbose=verbose)
        
        done = False
        truncated = False
        total_reward = 0
        steps = 0
        episode_alphas = []
        
        # Episodeå¾ªç¯
        while not (done or truncated) and steps < max_steps:
            # é€‰æ‹©åŠ¨ä½œ
            action, log_prob = trainer.select_action(state)
            
            # è·å–Î±æƒé‡
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).to(device)
                alpha = trainer.policy_net.get_alpha_weights(state_tensor)
                episode_alphas.append(alpha.cpu().numpy())
            
            if verbose and steps == 0:
                print(f"\n[2. ç½‘ç»œå†³ç­– - ç¬¬1æ­¥]")
                print(f"  4ä¸ªActoræŠ•ç¥¨:")
                print(f"    Î±æƒé‡: pre={alpha[0]:.3f}, scene={alpha[1]:.3f}, effect={alpha[2]:.3f}, rule={alpha[3]:.3f}")
                print(f"  é€‰æ‹©åŠ¨ä½œ: {action}")
            
            # æ‰§è¡ŒåŠ¨ä½œ
            obs, reward, done, truncated, info = env.step(action)
            
            if verbose and steps == 0:
                print(f"\n[3. æ‰§è¡ŒåŠ¨ä½œ]")
                print(f"  åŠ¨ä½œID: {action}")
                print(f"  å¥–åŠ±: {reward:.3f}")
                print(f"  å®Œæˆ: {done or truncated}")
            
            # æå–ä¸‹ä¸€çŠ¶æ€
            next_state = extract_state_from_nethack_obs(obs, state_constructor, verbose=False)
            
            # å­˜å‚¨ç»éªŒ
            trainer.store_transition(state, action, reward, next_state, done or truncated, log_prob)
            
            state = next_state
            total_reward += reward
            steps += 1
        
        # æ›´æ–°ç­–ç•¥
        if verbose:
            print(f"\n[4. å­¦ä¹ æ›´æ–°]")
            print(f"  æ”¶é›†ç»éªŒ: {len(trainer.buffer)}æ¡")
        
        update_stats = trainer.update()
        
        if verbose and update_stats:
            print(f"  Actor Loss: {update_stats.get('actor_loss', 0):.4f}")
            print(f"  Critic Loss: {update_stats.get('critic_loss', 0):.4f}")
            print(f"  å¹³å‡ä¼˜åŠ¿: {update_stats.get('avg_advantage', 0):.4f}")
        
        # è®°å½•ç»Ÿè®¡
        episode_rewards.append(total_reward)
        episode_lengths.append(steps)
        final_score = (
            obs.get('blstats', [0] * nh.NLE_BLSTATS_SIZE)[nh.NLE_BL_SCORE] if isinstance(obs, dict) else 0
        )
        episode_scores.append(final_score)
        
        if episode_alphas:
            avg_alpha = np.mean(episode_alphas, axis=0)
            alpha_history.append(avg_alpha)
        
        if total_reward > best_reward:
            best_reward = total_reward
            trainer.save_checkpoint(str(output_dir / "checkpoints" / "best_model.pth"))
            if verbose:
                print(f"\n  ğŸ‰ æ–°æœ€ä½³å¥–åŠ±: {best_reward:.2f}")
        
        if final_score > best_score:
            best_score = final_score
            if verbose:
                print(f"  ğŸ‰ æ–°æœ€ä½³åˆ†æ•°: {best_score:.0f}")
        
        # å®šæœŸä¿å­˜
        if (episode + 1) % checkpoint_interval == 0:
            trainer.save_checkpoint(str(output_dir / "checkpoints" / f"model_{episode+1:05d}.pth"))
            print(f"\nğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: model_{episode+1:05d}.pth")
        
        # è¯„ä¼°ç»Ÿè®¡
        if (episode + 1) % eval_interval == 0:
            avg_reward = np.mean(episode_rewards[-eval_interval:])
            avg_length = np.mean(episode_lengths[-eval_interval:])
            avg_score = np.mean(episode_scores[-eval_interval:])
            
            if len(alpha_history) >= eval_interval:
                recent_alphas = np.array(alpha_history[-eval_interval:])
                avg_alpha = recent_alphas.mean(axis=0)
                
                print(f"\n{'â”€'*80}")
                print(f"ğŸ“Š è¯„ä¼°ç»Ÿè®¡ [Episode {episode+1}]")
                print(f"{'â”€'*80}")
                print(f"  å¥–åŠ±: å¹³å‡={avg_reward:.2f}, æœ€ä½³={best_reward:.2f}")
                print(f"  åˆ†æ•°: å¹³å‡={avg_score:.0f}, æœ€ä½³={best_score:.0f}")
                print(f"  é•¿åº¦: {avg_length:.0f}æ­¥")
                print(f"  Î±æƒé‡: pre={avg_alpha[0]:.3f}, scene={avg_alpha[1]:.3f}, effect={avg_alpha[2]:.3f}, rule={avg_alpha[3]:.3f}")
                
                elapsed = time.time() - start_time
                eps_per_sec = (episode + 1) / elapsed
                print(f"  é€Ÿåº¦: {eps_per_sec:.2f} eps/s")
                print(f"  å·²ç”¨æ—¶é—´: {elapsed/60:.1f}åˆ†é’Ÿ")
    
    env.close()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    trainer.save_checkpoint(str(output_dir / "checkpoints" / "model_final.pth"))
    
    # ä¿å­˜æ—¥å¿—
    log_data = {
        'episode_rewards': episode_rewards,
        'episode_lengths': episode_lengths,
        'episode_scores': episode_scores,
        'alpha_history': [a.tolist() for a in alpha_history],
        'best_reward': float(best_reward),
        'best_score': float(best_score),
        'total_episodes': num_episodes,
        'total_time_seconds': time.time() - start_time,
        'device': str(device),
        'timestamp': datetime.now().isoformat(),
    }
    
    with open(output_dir / "logs" / "training_log.json", 'w') as f:
        json.dump(log_data, f, indent=2)
    
    # æœ€ç»ˆæŠ¥å‘Š
    if alpha_history:
        alpha_array = np.array(alpha_history)
        alpha_mean = alpha_array.mean(axis=0)
        alpha_std = alpha_array.std(axis=0)
        
        print_section("è®­ç»ƒå®Œæˆ")
        print(f"æ€»æ—¶é—´: {(time.time() - start_time)/60:.1f}åˆ†é’Ÿ")
        print(f"æœ€ä½³å¥–åŠ±: {best_reward:.2f}")
        print(f"æœ€ä½³åˆ†æ•°: {best_score:.0f}")
        print(f"å¹³å‡å¥–åŠ±: {np.mean(episode_rewards):.2f}")
        print(f"å¹³å‡åˆ†æ•°: {np.mean(episode_scores):.0f}")
        print(f"\nÎ±æƒé‡åˆ†å¸ƒ:")
        print(f"  Î±_pre:    {alpha_mean[0]:.3f} Â± {alpha_std[0]:.3f}")
        print(f"  Î±_scene:  {alpha_mean[1]:.3f} Â± {alpha_std[1]:.3f}")
        print(f"  Î±_effect: {alpha_mean[2]:.3f} Â± {alpha_std[2]:.3f}")
        print(f"  Î±_rule:   {alpha_mean[3]:.3f} Â± {alpha_std[3]:.3f}")
        print(f"\næ£€æŸ¥ç‚¹: results/checkpoints/")
        print(f"æ—¥å¿—: results/logs/training_log.json")


if __name__ == "__main__":
    main()
